{"meta":{"version":1,"warehouse":"4.0.2"},"models":{"Asset":[{"_id":"source/CNAME","path":"CNAME","modified":1,"renderable":0},{"_id":"themes/butterfly/source/css/index.styl","path":"css/index.styl","modified":1,"renderable":1},{"_id":"themes/butterfly/source/css/var.styl","path":"css/var.styl","modified":1,"renderable":1},{"_id":"themes/butterfly/source/img/404.jpg","path":"img/404.jpg","modified":1,"renderable":1},{"_id":"themes/butterfly/source/img/favicon.png","path":"img/favicon.png","modified":1,"renderable":1},{"_id":"themes/butterfly/source/img/friend_404.gif","path":"img/friend_404.gif","modified":1,"renderable":1},{"_id":"themes/butterfly/source/img/th.jpeg","path":"img/th.jpeg","modified":1,"renderable":1},{"_id":"themes/butterfly/source/js/main.js","path":"js/main.js","modified":1,"renderable":1},{"_id":"themes/butterfly/source/js/tw_cn.js","path":"js/tw_cn.js","modified":1,"renderable":1},{"_id":"themes/butterfly/source/js/utils.js","path":"js/utils.js","modified":1,"renderable":1},{"_id":"themes/butterfly/source/js/search/algolia.js","path":"js/search/algolia.js","modified":1,"renderable":1},{"_id":"themes/butterfly/source/js/search/local-search.js","path":"js/search/local-search.js","modified":1,"renderable":1},{"_id":"themes/butterfly/source/img/page/996/img.png","path":"img/page/996/img.png","modified":1,"renderable":1},{"_id":"themes/butterfly/source/img/page/996/img_1.png","path":"img/page/996/img_1.png","modified":1,"renderable":1},{"_id":"themes/butterfly/source/img/page/996/img_2.png","path":"img/page/996/img_2.png","modified":1,"renderable":1},{"_id":"themes/butterfly/source/img/page/996/img_3.png","path":"img/page/996/img_3.png","modified":1,"renderable":1},{"_id":"themes/butterfly/source/img/page/996/img_4.png","path":"img/page/996/img_4.png","modified":1,"renderable":1}],"Cache":[{"_id":"source/.DS_Store","hash":"1f61b0d3fbe7084ff0fa73d61f31e09871a9598e","modified":1685158805412},{"_id":"source/CNAME","hash":"52b86eb178dd876811852c430419dbce2fbc2529","modified":1685172169215},{"_id":"source/_posts/System.currentTimeMillis性能问题测试与处理.md","hash":"da85ca71de6800506a377c91de827a9bd4f87b75","modified":1685170616010},{"_id":"source/_posts/Java进程间文件锁.md","hash":"3148ac0003beb5371e28e12e87e8a8b70549cfd7","modified":1557122606727},{"_id":"source/_posts/Spring事务基本概念.md","hash":"c2010329fcab104b4faef1beb6a974a1245eac1a","modified":1569811049052},{"_id":"source/_posts/什么是好的设计.md","hash":"e6ba01e759f785fac3713aaaca29f39a95b15237","modified":1685171214116},{"_id":"source/_posts/websocket构建站内通知.md","hash":"69d4e64d58e6b449b74a3b1acb6f4e0d5a3539e0","modified":1557124920022},{"_id":"source/_posts/为什么中国的996干不过美国的955.md","hash":"8832117a07f7ef2eb68a67dd2393c94a2a05ab23","modified":1685160759266},{"_id":"source/_posts/使用RateLimiter限制访问频率.md","hash":"363bb840266d98e3d866fe71da5cc4639a668d71","modified":1569744296769},{"_id":"themes/butterfly/README_CN.md","hash":"e19021371184361261ddef1d98eb308d78922714","modified":1685158805543},{"_id":"themes/butterfly/.DS_Store","hash":"c5fc24d620a29eae4880c5e19ec0ff688df6d281","modified":1685158805420},{"_id":"themes/butterfly/LICENSE","hash":"1128f8f91104ba9ef98d37eea6523a888dcfa5de","modified":1685158805421},{"_id":"themes/butterfly/README.md","hash":"52967a864c244af4db8c63902586cb617ee5b8aa","modified":1685158805542},{"_id":"themes/butterfly/languages/en.yml","hash":"4e9cdb7a3570929bcf082de7a4eac49140dddc73","modified":1685158805540},{"_id":"themes/butterfly/package.json","hash":"f19dc1e0af33b877bab3c2f50257430ccfcc064d","modified":1685158805543},{"_id":"themes/butterfly/_config.yml","hash":"adc8f25ce9d499c18731b55735e5bc37262383ca","modified":1685158805544},{"_id":"themes/butterfly/languages/default.yml","hash":"4025c0ba440eb24705dd0293ca9ca84efb3105cc","modified":1685158805539},{"_id":"themes/butterfly/plugins.yml","hash":"acc74f24c7e94fe9ded264307a4201c9b410cd9b","modified":1685158805544},{"_id":"themes/butterfly/languages/zh-CN.yml","hash":"7dd849c3ba34986c57c764d9e36150b4bfffd2e9","modified":1685158805541},{"_id":"themes/butterfly/languages/zh-TW.yml","hash":"ee01e068f12dc33adfae5733824ea1255deb5ca6","modified":1685158805541},{"_id":"themes/butterfly/layout/category.pug","hash":"710708cfdb436bc875602abf096c919ccdf544db","modified":1685158805425},{"_id":"themes/butterfly/layout/archive.pug","hash":"a0c034c2d319320a54046805e80b58dc48b7e233","modified":1685158805423},{"_id":"themes/butterfly/layout/index.pug","hash":"e1c3146834c16e6077406180858add0a8183875a","modified":1685158805424},{"_id":"themes/butterfly/layout/page.pug","hash":"baf469784aef227e4cc840550888554588e87a13","modified":1685158805494},{"_id":"themes/butterfly/layout/tag.pug","hash":"0440f42569df2676273c026a92384fa7729bc4e9","modified":1685158805494},{"_id":"themes/butterfly/layout/post.pug","hash":"fc9f45252d78fcd15e4a82bfd144401cba5b169a","modified":1685158805424},{"_id":"themes/butterfly/layout/includes/404.pug","hash":"cb49f737aca272ccfeb62880bd651eccee72a129","modified":1685158805447},{"_id":"themes/butterfly/layout/includes/head.pug","hash":"dd9fde431add984330e3178e06a8d74705e7340e","modified":1685158805449},{"_id":"themes/butterfly/layout/includes/footer.pug","hash":"02390a5b6ae1f57497b22ba2e6be9f13cfb7acac","modified":1685158805455},{"_id":"themes/butterfly/layout/includes/pagination.pug","hash":"4c85de4dea4dca4e5088097a79bd6d7009cbf8ef","modified":1685158805431},{"_id":"themes/butterfly/layout/includes/additional-js.pug","hash":"aca0ec7ef69b21d1f242c62fed389468a0f0e1a2","modified":1685158805449},{"_id":"themes/butterfly/source/.DS_Store","hash":"84f35e390633eadc3c78584a28f5f5f8ce7f43a5","modified":1685158805496},{"_id":"themes/butterfly/layout/includes/layout.pug","hash":"7fa9ae4b70b87fc97e992dde5944681f92b59bea","modified":1685158805456},{"_id":"themes/butterfly/scripts/events/404.js","hash":"83cd7f73225ccad123afbd526ce1834eb1eb6a6d","modified":1685158805547},{"_id":"themes/butterfly/layout/includes/sidebar.pug","hash":"8d39473ed112d113674a0f689f63fae06c72abd2","modified":1685158805489},{"_id":"themes/butterfly/layout/includes/rightside.pug","hash":"83a1f2d31792206d432e8e2041e284d88327c02e","modified":1685158805448},{"_id":"themes/butterfly/scripts/events/cdn.js","hash":"21fb5aabe043486d095c4c8cce361ed85ba88a26","modified":1685158805550},{"_id":"themes/butterfly/scripts/events/init.js","hash":"3ace1139182d3d367149db138990891427f3356e","modified":1685158805548},{"_id":"themes/butterfly/scripts/events/stylus.js","hash":"9819f0996234fbd80d6c50a9e526c56ebf22588d","modified":1685158805549},{"_id":"themes/butterfly/scripts/events/welcome.js","hash":"8ad9911b755cba13dde2cc055c3f857a6b0dd20e","modified":1685158805549},{"_id":"themes/butterfly/scripts/events/comment.js","hash":"5351e0bc09e6b5b3f6d30f333a2520626a28ca3a","modified":1685158805548},{"_id":"themes/butterfly/scripts/filters/post_lazyload.js","hash":"932df912976261929f809b7dbd4eb473e7787345","modified":1685158805546},{"_id":"themes/butterfly/scripts/filters/random_cover.js","hash":"8d25f47434deae870bbffd07efe528a40363ab4d","modified":1685158805546},{"_id":"themes/butterfly/scripts/tag/button.js","hash":"91d954f6e9fe6e571eb8ec9f8996294b2dc3688e","modified":1685158805554},{"_id":"themes/butterfly/scripts/tag/flink.js","hash":"ab62919fa567b95fbe14889517abda649991b1ee","modified":1685158805551},{"_id":"themes/butterfly/scripts/tag/gallery.js","hash":"950b3dbac0b21717458a8d1769cbfc454d0eff54","modified":1685158805553},{"_id":"themes/butterfly/scripts/tag/hide.js","hash":"396c3ab1bcf1c7693ad7e506eadd13016c6769b6","modified":1685158805555},{"_id":"themes/butterfly/scripts/tag/label.js","hash":"03b2afef41d02bd1045c89578a02402c28356006","modified":1685158805555},{"_id":"themes/butterfly/scripts/tag/mermaid.js","hash":"531808a290b8bdd66bac2faab211ada8e9646a37","modified":1685158805550},{"_id":"themes/butterfly/scripts/tag/inlineImg.js","hash":"a43ee2c7871bdd93cb6beb804429e404570f7929","modified":1685158805551},{"_id":"themes/butterfly/scripts/tag/note.js","hash":"d51812b43924f1bbf413c67499510dd125022005","modified":1685158805554},{"_id":"themes/butterfly/scripts/tag/score.js","hash":"ea679dfe12d0e2290113b4a9d00663ce7a5ee5ad","modified":1685158805553},{"_id":"themes/butterfly/scripts/tag/tabs.js","hash":"6c6e415623d0fd39da016d9e353bb4f5cca444f5","modified":1685158805552},{"_id":"themes/butterfly/scripts/helpers/aside_archives.js","hash":"2ec66513d5322f185d2071acc052978ba9415a8e","modified":1685158805557},{"_id":"themes/butterfly/scripts/tag/timeline.js","hash":"300eb779588bf35a1b687d9f829d866074b707e3","modified":1685158805552},{"_id":"themes/butterfly/scripts/helpers/aside_categories.js","hash":"e00efdb5d02bc5c6eb4159e498af69fa61a7dbb9","modified":1685158805558},{"_id":"themes/butterfly/scripts/helpers/findArchiveLength.js","hash":"9ea86bd7a3c3fca3324f70b1cd4d9e42f9efb08d","modified":1685158805557},{"_id":"themes/butterfly/scripts/helpers/page.js","hash":"ce5d5a3d07b0d76ac5e96e5f9e5783f4b601b6be","modified":1685158805556},{"_id":"themes/butterfly/scripts/helpers/related_post.js","hash":"4677be4175da6800c0b3b8c1614e593f73df8831","modified":1685158805556},{"_id":"themes/butterfly/scripts/helpers/inject_head_js.js","hash":"4238e06ff448ff2ee717cd4c874f37f04d35da06","modified":1685158805558},{"_id":"themes/butterfly/source/css/index.styl","hash":"861998e4ac67a59529a8245a9130d68f826c9c12","modified":1685158805506},{"_id":"themes/butterfly/source/css/var.styl","hash":"30abbb8eed880d51f61f336064d93abd709e0115","modified":1685158805504},{"_id":"themes/butterfly/source/img/favicon.png","hash":"3cf89864b4f6c9b532522a4d260a2e887971c92d","modified":1685158805535},{"_id":"themes/butterfly/source/js/main.js","hash":"dca55899b7c92ebee6191bef127ea5e2283ecc63","modified":1685158805522},{"_id":"themes/butterfly/layout/includes/head/Open_Graph.pug","hash":"8aa8d799aedbfd811195b84a451bc4b6e2647c12","modified":1685158805451},{"_id":"themes/butterfly/source/img/404.jpg","hash":"fb4489bc1d30c93d28f7332158c1c6c1416148de","modified":1685158805534},{"_id":"themes/butterfly/source/js/utils.js","hash":"7bec147ae2b313fbd0331d87018f99715368fc4b","modified":1685158805523},{"_id":"themes/butterfly/layout/includes/head/analytics.pug","hash":"67e1c3b48e4ca7ee0b2c76d3ca7476b9883cf105","modified":1685158805453},{"_id":"themes/butterfly/layout/includes/head/config.pug","hash":"75e7a524af64fbaaaf7b05a1b1922bf6940d7afe","modified":1685158805451},{"_id":"themes/butterfly/layout/includes/head/pwa.pug","hash":"3d492cfe645d37c94d30512e0b230b0a09913148","modified":1685158805452},{"_id":"themes/butterfly/source/img/friend_404.gif","hash":"8d2d0ebef70a8eb07329f57e645889b0e420fa48","modified":1685158805538},{"_id":"themes/butterfly/layout/includes/head/preconnect.pug","hash":"a03b3ddc06e7aa9fd07eea0d5f97c8d5addd2315","modified":1685158805452},{"_id":"themes/butterfly/layout/includes/head/noscript.pug","hash":"d16ad2ee0ff5751fd7f8a5ce1b83935518674977","modified":1685158805453},{"_id":"themes/butterfly/layout/includes/head/config_site.pug","hash":"7df90c8e432e33716517ab918b0a125bc284041b","modified":1685158805454},{"_id":"themes/butterfly/source/js/tw_cn.js","hash":"42b106354d72a0ea1fe62587b313a5b7de3cc393","modified":1685158805524},{"_id":"themes/butterfly/layout/includes/head/site_verification.pug","hash":"e2e8d681f183f00ce5ee239c42d2e36b3744daad","modified":1685158805454},{"_id":"themes/butterfly/layout/includes/header/index.pug","hash":"0c1551ef80bbece550fe520d91e21f083cbc14fe","modified":1685158805491},{"_id":"themes/butterfly/layout/includes/head/google_adsense.pug","hash":"95a37e92b39c44bcbea4be7e29ddb3921c5b8220","modified":1685158805454},{"_id":"themes/butterfly/layout/includes/header/social.pug","hash":"5de9a82032cdad1db3b868b797460921cd775fc2","modified":1685158805493},{"_id":"themes/butterfly/layout/includes/header/menu_item.pug","hash":"31346a210f4f9912c5b29f51d8f659913492f388","modified":1685158805492},{"_id":"themes/butterfly/layout/includes/loading/fullpage-loading.pug","hash":"68cda524337dfe2e1467318a4a6c124b4c3845a7","modified":1685158805436},{"_id":"themes/butterfly/layout/includes/header/nav.pug","hash":"f61659aa457d1a2d1baa3a13157996cfac4d6609","modified":1685158805490},{"_id":"themes/butterfly/layout/includes/header/post-info.pug","hash":"9698f22751778dde063cbfbd01c59ca4462ccd85","modified":1685158805493},{"_id":"themes/butterfly/layout/includes/loading/index.pug","hash":"131f344d68b4c241d6e03849b243ee792fcd3cea","modified":1685158805434},{"_id":"themes/butterfly/layout/includes/loading/pace.pug","hash":"6ab4e301c92586505d6cddce1b3ad23b7c79010d","modified":1685158805435},{"_id":"themes/butterfly/layout/includes/mixins/post-ui.pug","hash":"294df7a74cf36af3a7030274d8b745979c1c8c70","modified":1685158805448},{"_id":"themes/butterfly/layout/includes/page/default-page.pug","hash":"12c65c174d26a41821df9bad26cdf1087ec5b0ca","modified":1685158805428},{"_id":"themes/butterfly/layout/includes/mixins/article-sort.pug","hash":"90554c2ca5ba946f4c02e1bc5fe2859cef1b1594","modified":1685158805447},{"_id":"themes/butterfly/layout/includes/page/categories.pug","hash":"5276a8d2835e05bd535fedc9f593a0ce8c3e8437","modified":1685158805430},{"_id":"themes/butterfly/layout/includes/page/flink.pug","hash":"a59bcfbb609a099c1bf5be40b7a94e7e2b06fc4a","modified":1685158805428},{"_id":"themes/butterfly/layout/includes/third-party/aplayer.pug","hash":"c7cfade2b160380432c47eef4cd62273b6508c58","modified":1685158805463},{"_id":"themes/butterfly/layout/includes/page/tags.pug","hash":"9621991359e22b14049346f1cf87bdedc94edf5a","modified":1685158805429},{"_id":"themes/butterfly/layout/includes/post/reward.pug","hash":"864869c43fe5b5bb6f4ac6b13dd4bfb16ea47550","modified":1685158805433},{"_id":"themes/butterfly/layout/includes/post/post-copyright.pug","hash":"ebecba46a5f4efe1c98a386df06c56e26fbd07b9","modified":1685158805432},{"_id":"themes/butterfly/layout/includes/third-party/effect.pug","hash":"6528e86656906117a1af6b90e0349c2c4651d5e1","modified":1685158805466},{"_id":"themes/butterfly/layout/includes/third-party/pjax.pug","hash":"fc0b09068009edd4026d90a669608cbe211aeecf","modified":1685158805484},{"_id":"themes/butterfly/layout/includes/third-party/prismjs.pug","hash":"ffb9ea15a2b54423cd4cd441e2d061b8233e9b58","modified":1685158805478},{"_id":"themes/butterfly/layout/includes/third-party/subtitle.pug","hash":"13015a98d0d5e1ef2cec294231529010395b19de","modified":1685158805466},{"_id":"themes/butterfly/layout/includes/third-party/pangu.pug","hash":"0f024e36b8116118233e10118714bde304e01e12","modified":1685158805475},{"_id":"themes/butterfly/layout/includes/widget/card_announcement.pug","hash":"ae392459ad401a083ca51ee0b27526b3c1e1faed","modified":1685158805440},{"_id":"themes/butterfly/layout/includes/widget/card_ad.pug","hash":"60dc48a7b5d89c2a49123c3fc5893ab9c57dd225","modified":1685158805443},{"_id":"themes/butterfly/layout/includes/widget/card_archives.pug","hash":"86897010fe71503e239887fd8f6a4f5851737be9","modified":1685158805442},{"_id":"themes/butterfly/layout/includes/widget/card_author.pug","hash":"e37468e63db2a0ac09b65d21b7de3e62425bb455","modified":1685158805445},{"_id":"themes/butterfly/layout/includes/widget/card_bottom_self.pug","hash":"13dc8ce922e2e2332fe6ad5856ebb5dbf9ea4444","modified":1685158805441},{"_id":"themes/butterfly/layout/includes/widget/card_categories.pug","hash":"d1a416d0a8a7916d0b1a41d73adc66f8c811e493","modified":1685158805445},{"_id":"themes/butterfly/layout/includes/widget/card_recent_post.pug","hash":"e5aac7b28ed4123d75797263c64e74ac547945bc","modified":1685158805438},{"_id":"themes/butterfly/layout/includes/widget/card_newest_comment.pug","hash":"6d93564a8bd13cb9b52ee5e178db3bcbf18b1bc6","modified":1685158805444},{"_id":"themes/butterfly/layout/includes/widget/card_post_toc.pug","hash":"3057a2f6f051355e35d3b205121af8735100eacf","modified":1685158805438},{"_id":"themes/butterfly/layout/includes/widget/card_top_self.pug","hash":"ae67c6d4130a6c075058a9c1faea1648bcc6f83e","modified":1685158805446},{"_id":"themes/butterfly/layout/includes/widget/card_tags.pug","hash":"eceb4420a64c720f0d2741e89d6229bbb3d87353","modified":1685158805444},{"_id":"themes/butterfly/source/css/_mode/darkmode.styl","hash":"7ff0c456fae2717ddbbb9f8fae2734d449a5448b","modified":1685158805502},{"_id":"themes/butterfly/layout/includes/widget/index.pug","hash":"010e3d548ababca2280c4fc4168d9a4a1ee4f536","modified":1685158805439},{"_id":"themes/butterfly/layout/includes/widget/card_webinfo.pug","hash":"35ce167c5a275211bfc1fa3d49adfde5b404d98f","modified":1685158805442},{"_id":"themes/butterfly/source/css/_mode/readmode.styl","hash":"ffea9e7c1543edcf080381e7b99828954c2f2cef","modified":1685158805501},{"_id":"themes/butterfly/source/css/_global/function.styl","hash":"7fdfbe8f97b41588bbd5c6f27e7e85a881b28954","modified":1685158805503},{"_id":"themes/butterfly/source/css/_global/index.styl","hash":"de10b113f9cb9a68d257a39d6409905acfd559ef","modified":1685158805503},{"_id":"themes/butterfly/source/css/_layout/aside.styl","hash":"57adf29a3e36e4ea84384e36c034eb294dffb208","modified":1685158805511},{"_id":"themes/butterfly/source/css/_layout/footer.styl","hash":"83553445fbc92cad4ad220fbd87b4c3db958c32a","modified":1685158805510},{"_id":"themes/butterfly/source/css/_layout/chat.styl","hash":"f9a5d3f1fc5ed0ed2ee4c1eaa58ed650d11ddebd","modified":1685158805511},{"_id":"themes/butterfly/source/css/_layout/comments.styl","hash":"c61dccca690d486c3d9c29cf028d87b777385141","modified":1685158805509},{"_id":"themes/butterfly/source/css/_layout/head.styl","hash":"45d71dbb2a61e30989851ba29bb8be7094574d14","modified":1685158805512},{"_id":"themes/butterfly/source/css/_layout/loading.styl","hash":"ac2aeee9926f75b2a0098efe1c114126987430f2","modified":1685158805510},{"_id":"themes/butterfly/source/css/_layout/pagination.styl","hash":"fb9f78bfbb79579f1d752cb73fb6d25c8418e0fd","modified":1685158805508},{"_id":"themes/butterfly/source/css/_layout/relatedposts.styl","hash":"d53de408cb27a2e704aba7f7402b7caebe0410d8","modified":1685158805509},{"_id":"themes/butterfly/source/css/_layout/reward.styl","hash":"c5cfed620708807a48076b5ee59b0ba84e29aa80","modified":1685158805511},{"_id":"themes/butterfly/source/css/_layout/post.styl","hash":"e24046fad288a13897195038cb7a63d1014cd7b8","modified":1685158805508},{"_id":"themes/butterfly/source/css/_layout/rightside.styl","hash":"bbc884d6b2158a833b77a1bbc07248e17874b22e","modified":1685158805508},{"_id":"themes/butterfly/source/css/_layout/sidebar.styl","hash":"631ca35a38bc4ac052e9caf47508ff1f99842fc7","modified":1685158805511},{"_id":"themes/butterfly/source/css/_layout/third-party.styl","hash":"ca39e634668ed4fbb43267ec4782c2b55c44e698","modified":1685158805510},{"_id":"themes/butterfly/source/css/_highlight/highlight.styl","hash":"57a19eb0c418d92a88b143f56ccb8cd60e6d7ad0","modified":1685158805519},{"_id":"themes/butterfly/source/css/_highlight/theme.styl","hash":"bcd384c8b2aa0390c9eb69ac1abbfd1240ce1da4","modified":1685158805519},{"_id":"themes/butterfly/source/css/_page/404.styl","hash":"50dbb9e6d98c71ffe16741b8c1b0c1b9771efd2b","modified":1685158805498},{"_id":"themes/butterfly/source/css/_page/archives.styl","hash":"c9e98027f2dd730ce389c2047f62ebb748955fcf","modified":1685158805499},{"_id":"themes/butterfly/source/css/_page/categories.styl","hash":"f01ee74948cedb44e53cd3bb1ef36b7d2778ede7","modified":1685158805500},{"_id":"themes/butterfly/source/css/_page/common.styl","hash":"e4b9d6164e97b30c84e1218c7543c60f6b29edcc","modified":1685158805500},{"_id":"themes/butterfly/source/css/_page/homepage.styl","hash":"bb470da1d2ba292cae0a30a252f82f37c4130d2d","modified":1685158805498},{"_id":"themes/butterfly/source/css/_page/flink.styl","hash":"98d755b686ee833e9da10afaa40c4ec2bd66c19a","modified":1685158805499},{"_id":"themes/butterfly/source/css/_page/tags.styl","hash":"580feb7e8b0822a1be48ac380f8c5c53b1523321","modified":1685158805498},{"_id":"themes/butterfly/source/css/_tags/gallery.styl","hash":"81ad85acf0e0fe7f9ee23c16a700e7154574d5dd","modified":1685158805515},{"_id":"themes/butterfly/source/css/_tags/button.styl","hash":"45f0c32bdea117540f6b14ebac6450d7142bd710","modified":1685158805513},{"_id":"themes/butterfly/source/css/_tags/hexo.styl","hash":"d76c38adf1d9c1279ef4241835667789f5b736e0","modified":1685158805514},{"_id":"themes/butterfly/source/css/_tags/inlineImg.styl","hash":"df9d405c33a9a68946b530410f64096bcb72560c","modified":1685158805514},{"_id":"themes/butterfly/source/css/_tags/hide.styl","hash":"ce489ca2e249e2a3cf71584e20d84bdb022e3475","modified":1685158805515},{"_id":"themes/butterfly/source/css/_tags/tabs.styl","hash":"bf9568444dd54e39dc59b461323dcd38942f27d9","modified":1685158805513},{"_id":"themes/butterfly/source/css/_tags/label.styl","hash":"66c59e193d794cdb02cca7bd1dc4aea5a19d7e84","modified":1685158805514},{"_id":"themes/butterfly/source/css/_search/algolia.styl","hash":"649a054e73278b6724bd4dd9b94724791ec5c928","modified":1685158805506},{"_id":"themes/butterfly/source/css/_search/index.styl","hash":"678e56ad2e46b630364540fc6a881d6801192dcd","modified":1685158805505},{"_id":"themes/butterfly/source/css/_tags/note.styl","hash":"85ae91c83691ea4511f4277da1194a185251cc78","modified":1685158805515},{"_id":"themes/butterfly/source/css/_tags/timeline.styl","hash":"f071156d439556e7463ed4bc61ceee87170d5d08","modified":1685158805513},{"_id":"themes/butterfly/source/css/_search/local-search.styl","hash":"a86e4e9198b225b4b73a7a45f04b86cbbed0d231","modified":1685158805505},{"_id":"themes/butterfly/source/css/_third-party/normalize.min.css","hash":"2c18a1c9604af475b4749def8f1959df88d8b276","modified":1685158805519},{"_id":"themes/butterfly/layout/includes/third-party/abcjs/abcjs.pug","hash":"ed6906b7c6aa7046bbad95dfdda9211997be7099","modified":1685158805477},{"_id":"themes/butterfly/layout/includes/third-party/abcjs/index.pug","hash":"f58f1648d2d71311bafca4833f20b605bb5f18c8","modified":1685158805476},{"_id":"themes/butterfly/layout/includes/third-party/card-post-count/fb.pug","hash":"0344477a2cf38698318ead2681c63ac12f01586e","modified":1685158805481},{"_id":"themes/butterfly/layout/includes/third-party/card-post-count/artalk.pug","hash":"4ff8e67cd1c9058c0c894737b9b247a812079ae2","modified":1685158805483},{"_id":"themes/butterfly/layout/includes/third-party/card-post-count/disqus.pug","hash":"d85c3737b5c9548553a78b757a7698df126a52cf","modified":1685158805481},{"_id":"themes/butterfly/layout/includes/third-party/card-post-count/index.pug","hash":"b2d274db84ef22fbd6d5ea8f4404821898934209","modified":1685158805479},{"_id":"themes/butterfly/source/js/search/algolia.js","hash":"fd86281d4f0f99ce173e49c1a0df3507fe268d37","modified":1685158805522},{"_id":"themes/butterfly/source/js/search/local-search.js","hash":"8509cbd954ee9e099dcfbbfdafba70893a56e9ae","modified":1685158805521},{"_id":"themes/butterfly/layout/includes/third-party/card-post-count/twikoo.pug","hash":"56c028ba0ea8fac19f0125114d765dfc56ce2b48","modified":1685158805484},{"_id":"themes/butterfly/layout/includes/third-party/card-post-count/remark42.pug","hash":"001e8be47854b891efe04013c240c38fed4185eb","modified":1685158805482},{"_id":"themes/butterfly/layout/includes/third-party/card-post-count/waline.pug","hash":"55acc455ca8e13211e3906cf78e487cc92accee5","modified":1685158805479},{"_id":"themes/butterfly/layout/includes/third-party/card-post-count/valine.pug","hash":"39427e107230a10790972349c9dd4c4f31d55eb7","modified":1685158805480},{"_id":"themes/butterfly/layout/includes/third-party/chat/chatra.pug","hash":"481cd5053bafb1a19f623554a27d3aa077ea59c3","modified":1685158805464},{"_id":"themes/butterfly/layout/includes/third-party/chat/crisp.pug","hash":"76634112c64023177260d1317ae39cef2a68e35f","modified":1685158805465},{"_id":"themes/butterfly/layout/includes/third-party/chat/daovoice.pug","hash":"cfe63e7d26a6665df6aa32ca90868ad48e05ec04","modified":1685158805464},{"_id":"themes/butterfly/layout/includes/third-party/chat/index.pug","hash":"618e1b7f9204049b07beb9e1363c844a78a9ace3","modified":1685158805464},{"_id":"themes/butterfly/layout/includes/third-party/chat/messenger.pug","hash":"3ce0461534b786cb71d9141dff35fa5cb70e22b9","modified":1685158805465},{"_id":"themes/butterfly/layout/includes/third-party/chat/tidio.pug","hash":"24a926756c2300b9c561aaab6bd3a71fdd16e16d","modified":1685158805465},{"_id":"themes/butterfly/layout/includes/third-party/comments/artalk.pug","hash":"90779382c8675e5a0b2d1ef2250294de926707f4","modified":1685158805462},{"_id":"themes/butterfly/layout/includes/third-party/comments/disqus.pug","hash":"9ef303da16d180619da18b146ddb9bc35f66bdbf","modified":1685158805460},{"_id":"themes/butterfly/layout/includes/third-party/comments/disqusjs.pug","hash":"6e17b2cea503eabeb328835038812cfa95f15871","modified":1685158805459},{"_id":"themes/butterfly/layout/includes/third-party/comments/facebook_comments.pug","hash":"0704efed9079c867ab5f7bee7381a6c869154c73","modified":1685158805462},{"_id":"themes/butterfly/layout/includes/third-party/comments/giscus.pug","hash":"5127bc550a2edb1ab9f45416e1964c76e8201544","modified":1685158805459},{"_id":"themes/butterfly/layout/includes/third-party/comments/gitalk.pug","hash":"2fc5627eb63118c83df9422b47c801822e28df98","modified":1685158805460},{"_id":"themes/butterfly/layout/includes/third-party/comments/index.pug","hash":"351fe25fbf02635b1f9e86e5e244c7d61f69baa7","modified":1685158805457},{"_id":"themes/butterfly/layout/includes/third-party/comments/js.pug","hash":"00ed91c52939b9675b316137f854d13684c895a6","modified":1685158805461},{"_id":"themes/butterfly/layout/includes/third-party/comments/livere.pug","hash":"52ea8aa26b84d3ad38ae28cdf0f163e9ca8dced7","modified":1685158805461},{"_id":"themes/butterfly/layout/includes/third-party/comments/twikoo.pug","hash":"4d78f6266d0870c06c10eaf47c951bd4d9a7732e","modified":1685158805462},{"_id":"themes/butterfly/layout/includes/third-party/comments/remark42.pug","hash":"598790433e3c9be28b0063bff08d257acd0abf75","modified":1685158805461},{"_id":"themes/butterfly/layout/includes/third-party/comments/valine.pug","hash":"e55b9c0f8ced231f47eb88bd7f4ec99f29c5c29d","modified":1685158805458},{"_id":"themes/butterfly/layout/includes/third-party/comments/utterances.pug","hash":"2a8d02ed9303092e8816f6489a443e7388102470","modified":1685158805460},{"_id":"themes/butterfly/layout/includes/third-party/math/mathjax.pug","hash":"0ea633b11b357afa50c200290d19c32467d58a1d","modified":1685158805467},{"_id":"themes/butterfly/layout/includes/third-party/math/katex.pug","hash":"dfcbd9881be569ea420eff1a6b00e4f4dbe2138e","modified":1685158805468},{"_id":"themes/butterfly/layout/includes/third-party/comments/waline.pug","hash":"26ba1fc99117993087b1c6e02daa2626627d8eb1","modified":1685158805458},{"_id":"themes/butterfly/layout/includes/third-party/math/mermaid.pug","hash":"d92bbf51626fcc5608b53273cf40db0b5b69c0d4","modified":1685158805468},{"_id":"themes/butterfly/layout/includes/third-party/math/index.pug","hash":"b8ae5fd7d74e1edcef21f5004fc96147e064d219","modified":1685158805467},{"_id":"themes/butterfly/layout/includes/third-party/newest-comments/disqus-comment.pug","hash":"4fe8faf77b8420fc031ae1b54f78b2ece9fcc07e","modified":1685158805475},{"_id":"themes/butterfly/layout/includes/third-party/newest-comments/github-issues.pug","hash":"0330e3063ccf0ce40e4828b8d4fbef62362e8195","modified":1685158805473},{"_id":"themes/butterfly/layout/includes/third-party/newest-comments/index.pug","hash":"4ec0642f2d5444acfab570a6f8c7868e7ff43fde","modified":1685158805471},{"_id":"themes/butterfly/layout/includes/third-party/newest-comments/artalk.pug","hash":"3942d6643683a3c42bbb5f4cf4a7df21debb8498","modified":1685158805474},{"_id":"themes/butterfly/layout/includes/third-party/newest-comments/remark42.pug","hash":"aa6061183a32472cd1882fce445a5049108a984b","modified":1685158805474},{"_id":"themes/butterfly/layout/includes/third-party/newest-comments/twikoo-comment.pug","hash":"44991d67abb81784c5cdb4337b2b9798fc4361e1","modified":1685158805473},{"_id":"themes/butterfly/layout/includes/third-party/newest-comments/waline.pug","hash":"531d7b6992a737742f7b3ae343e1f03aab947f4c","modified":1685158805472},{"_id":"themes/butterfly/layout/includes/third-party/search/docsearch.pug","hash":"1c3e101445c5571ba998ce293d3984319df1b3b0","modified":1685158805470},{"_id":"themes/butterfly/layout/includes/third-party/search/algolia.pug","hash":"9c3c109a12d2b6916e8b4965cca12f521510ead9","modified":1685158805469},{"_id":"themes/butterfly/layout/includes/third-party/newest-comments/valine.pug","hash":"b7b2aa5be4112065d5066c0f066f5f58721153bf","modified":1685158805472},{"_id":"themes/butterfly/layout/includes/third-party/search/index.pug","hash":"a99a41334387ee9a46c6f8e8212331a29a10d159","modified":1685158805469},{"_id":"themes/butterfly/layout/includes/third-party/search/local-search.pug","hash":"5ebd5e8d39c9f77f5b2d983f6cd6802ccaf98746","modified":1685158805470},{"_id":"themes/butterfly/layout/includes/third-party/share/add-this.pug","hash":"2980f1889226ca981aa23b8eb1853fde26dcf89a","modified":1685158805487},{"_id":"themes/butterfly/layout/includes/third-party/share/share-js.pug","hash":"c7dd2b2ae9b23aa0a60fffd7df9e9f76ef52033e","modified":1685158805488},{"_id":"themes/butterfly/layout/includes/third-party/share/index.pug","hash":"4c4a9c15215ae8ac5eadb0e086b278f76db9ee92","modified":1685158805486},{"_id":"themes/butterfly/layout/includes/third-party/share/addtoany.pug","hash":"85c92f8a7e44d7cd1c86f089a05be438535e5362","modified":1685158805487},{"_id":"themes/butterfly/source/css/_highlight/highlight/diff.styl","hash":"cf1fae641c927621a4df1be5ca4a853b9b526e23","modified":1685158805518},{"_id":"themes/butterfly/source/css/_highlight/highlight/index.styl","hash":"18804c58239d95798fa86d0597f32d7f7dd30051","modified":1685158805518},{"_id":"themes/butterfly/source/css/_highlight/prismjs/line-number.styl","hash":"8970cc1916c982b64a1478792b2822d1d31e276d","modified":1685158805517},{"_id":"themes/butterfly/source/css/_highlight/prismjs/diff.styl","hash":"5972c61f5125068cbe0af279a0c93a54847fdc3b","modified":1685158805517},{"_id":"themes/butterfly/source/css/_highlight/prismjs/index.styl","hash":"5dc2e0bcae9a54bfb9bdcc82d02ae5a3cf1ca97d","modified":1685158805516},{"_id":"themes/butterfly/source/img/page/996/img_2.png","hash":"b995e129e5e185e06284483142868a81ab4c220c","modified":1685158805530},{"_id":"themes/butterfly/source/img/page/996/img.png","hash":"4eae7c7c1b836cde8da598d3cf1dac3cd3eb90a6","modified":1685158805534},{"_id":"themes/butterfly/source/img/page/996/img_3.png","hash":"071d1b698e3e5ab95f392c8110dae558fc7e2992","modified":1685158805531},{"_id":"themes/butterfly/source/img/th.jpeg","hash":"c80e9c29165bf024ea2fdde0215c00f7db3dcc71","modified":1685158805536},{"_id":"themes/butterfly/source/img/page/996/img_1.png","hash":"ce79e2e8e690e5b909eb07dcc26c69488fc690cd","modified":1685158805528},{"_id":"themes/butterfly/source/img/page/996/img_4.png","hash":"db4149126462b054f4a379c04ce15074b5567676","modified":1685158805533},{"_id":"public/2019/03/20/什么是好的设计/index.html","hash":"cec2736ddca7d0800d4a5ffd62dc1e565601c168","modified":1685172280062},{"_id":"public/2020/05/29/为什么中国的996干不过美国的955/index.html","hash":"6f3baefbb3447ca6b8e0bc5480ddf4c698568b4a","modified":1685172280062},{"_id":"public/2019/07/10/System.currentTimeMillis性能问题测试与处理/index.html","hash":"794a29522d5c8da84a0e9dbc7861cf15591aa5c8","modified":1685172280062},{"_id":"public/2019/06/12/使用RateLimiter限制访问频率/index.html","hash":"7e7614d238e45c750231644f84c2e14d97cb07d6","modified":1685172280062},{"_id":"public/2019/04/10/websocket构建站内通知/index.html","hash":"27cc23174549cf0ebd9cec00456b198fb63f965e","modified":1685172280062},{"_id":"public/2017/08/12/Java进程间文件锁/index.html","hash":"0cf16e539540a64ad6a9b8c0c363f49198841e9a","modified":1685172280062},{"_id":"public/2016/09/03/Spring事务基本概念/index.html","hash":"ffc5582598a1ebf566179ebd2039bc3a1fe591b3","modified":1685172280062},{"_id":"public/archives/index.html","hash":"1d59644d2f4590ff455927b9612b831120820412","modified":1685172280062},{"_id":"public/archives/2016/index.html","hash":"2dc31cf72fe9e13c7e10d2d1c96851956d8dee0c","modified":1685172280062},{"_id":"public/archives/2016/09/index.html","hash":"bbaae5afc220c0cf37bc5b341a4c268855760da1","modified":1685172280062},{"_id":"public/archives/2017/index.html","hash":"31f25688132f889345907f72ac0c11d49f4529a2","modified":1685172280062},{"_id":"public/archives/2017/08/index.html","hash":"dc82a8399e6fccc4a259fea2476ba7bd0c24987e","modified":1685172280062},{"_id":"public/archives/2019/index.html","hash":"debeb5d58732cbbafcd7a5d3a60dbc4bd1203a55","modified":1685172280062},{"_id":"public/archives/2019/03/index.html","hash":"a40f6dedf66b2316416d6eec5aede046b1dc0766","modified":1685172280062},{"_id":"public/archives/2019/04/index.html","hash":"e9538f91b0898be53d6a60102889188aec5b46b2","modified":1685172280062},{"_id":"public/archives/2019/06/index.html","hash":"a142c13ff1715cc759699c1452f319953a225eb3","modified":1685172280062},{"_id":"public/archives/2019/07/index.html","hash":"21eb8e8105f5976edcaec8b39e2b12457cc156d9","modified":1685172280062},{"_id":"public/archives/2020/index.html","hash":"d718bd0ceebd9caf0ba8d45947bb60ff3f7c9547","modified":1685172280062},{"_id":"public/archives/2020/05/index.html","hash":"fc93395b43e8e0c5aec7675884902b2456e915e9","modified":1685172280062},{"_id":"public/categories/Java/index.html","hash":"2ab776f0f73485349447c6898dda2f37b4acb5cc","modified":1685172280062},{"_id":"public/categories/spring/index.html","hash":"c779a491593331c231c2159ea16584aebfffcc0e","modified":1685172280062},{"_id":"public/categories/websocket/index.html","hash":"5c6f6df6b568bffadd6ced49cd582d55bd6f2ece","modified":1685172280062},{"_id":"public/categories/design/index.html","hash":"6abde219b5a8523c4b4622886a86cf7c6ee96aff","modified":1685172280062},{"_id":"public/categories/SaaS/index.html","hash":"7ceab3e9c384b6d749b6ab0090955b003372e0e3","modified":1685172280062},{"_id":"public/index.html","hash":"ac81c34d1506f9bc5ddb465bf7e1b15b6fd10e0e","modified":1685172280062},{"_id":"public/tags/lock-file/index.html","hash":"d5bc4576a795dd3f4205c25964e98cec40b0b62c","modified":1685172280062},{"_id":"public/tags/性能-时间戳/index.html","hash":"fc308ed6c0145ab00851cc48ff6a72778636910a","modified":1685172280062},{"_id":"public/tags/spring-transaction/index.html","hash":"d3b8421f8a16120bb071a64cfbe4ae52f127479f","modified":1685172280062},{"_id":"public/tags/websocket/index.html","hash":"1d2f64161b0121efc35c300482ba1564100d81c0","modified":1685172280062},{"_id":"public/tags/design-view/index.html","hash":"7cd962c145274f2e72f6fa261ef494b383b4e400","modified":1685172280062},{"_id":"public/tags/qps-ratelimiter/index.html","hash":"73e5aa60c6ac49470156a3c02158b4e3c2bb2ca3","modified":1685172280062},{"_id":"public/tags/saas-996/index.html","hash":"47f5e3191c0dc814aff16a4eaaad8304861a5fa0","modified":1685172280062},{"_id":"public/img/favicon.png","hash":"3cf89864b4f6c9b532522a4d260a2e887971c92d","modified":1685172280062},{"_id":"public/img/404.jpg","hash":"fb4489bc1d30c93d28f7332158c1c6c1416148de","modified":1685172280062},{"_id":"public/img/friend_404.gif","hash":"8d2d0ebef70a8eb07329f57e645889b0e420fa48","modified":1685172280062},{"_id":"public/CNAME","hash":"52b86eb178dd876811852c430419dbce2fbc2529","modified":1685172280062},{"_id":"public/img/page/996/img_2.png","hash":"b995e129e5e185e06284483142868a81ab4c220c","modified":1685172280062},{"_id":"public/css/var.css","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1685172280062},{"_id":"public/js/utils.js","hash":"7bec147ae2b313fbd0331d87018f99715368fc4b","modified":1685172280062},{"_id":"public/js/search/local-search.js","hash":"8509cbd954ee9e099dcfbbfdafba70893a56e9ae","modified":1685172280062},{"_id":"public/js/search/algolia.js","hash":"fd86281d4f0f99ce173e49c1a0df3507fe268d37","modified":1685172280062},{"_id":"public/css/index.css","hash":"2909ac961debcc749507ff42036e833d52207050","modified":1685172280062},{"_id":"public/js/main.js","hash":"dca55899b7c92ebee6191bef127ea5e2283ecc63","modified":1685172280062},{"_id":"public/js/tw_cn.js","hash":"42b106354d72a0ea1fe62587b313a5b7de3cc393","modified":1685172280062},{"_id":"public/img/page/996/img.png","hash":"4eae7c7c1b836cde8da598d3cf1dac3cd3eb90a6","modified":1685172280062},{"_id":"public/img/page/996/img_3.png","hash":"071d1b698e3e5ab95f392c8110dae558fc7e2992","modified":1685172280062},{"_id":"public/img/th.jpeg","hash":"c80e9c29165bf024ea2fdde0215c00f7db3dcc71","modified":1685172280062},{"_id":"public/img/page/996/img_1.png","hash":"ce79e2e8e690e5b909eb07dcc26c69488fc690cd","modified":1685172280062},{"_id":"public/img/page/996/img_4.png","hash":"db4149126462b054f4a379c04ce15074b5567676","modified":1685172280062}],"Category":[{"name":"Java","_id":"cli5o2dqo0002czzk03wyfpi5"},{"name":"spring","_id":"cli5o2dqy000cczzk104y5jt1"},{"name":"websocket","_id":"cli5o2dr0000hczzk3hvqag0i"},{"name":"design","_id":"cli5o2dr1000mczzkfide0ixr"},{"name":"SaaS","_id":"cli5o2dr2000qczzk6f58dbhq"}],"Data":[],"Page":[],"Post":[{"title":"Java进程间共享锁","date":"2017-08-12T03:47:44.000Z","description":"Java进程间共享锁","_content":"\n****背景****     \n业务系统运行时产生了大量的小文件（大小在10kb左右，包含一些表单的内容，rest接口的访问记录），考虑到将这些内容存储在数据库中对数据库压力较大，因此转为文件存储，但由于restful接口访问频繁，导致产生了大量的小文件，最终导致服务器inode占用率过高，触发告警\n\n\n****原因****    \n\n什么是inode?\ninode包含文件的元信息，具体来说有以下内容：\n\n　　* 文件的字节数\n\n　　* 文件拥有者的User ID\n\n　　* 文件的Group ID\n\n　　* 文件的读、写、执行权限\n\n　　* 文件的时间戳，共有三个：ctime指inode上一次变动的时间，mtime指文件内容上一次变动的时间，atime指文件上一次打开的时间。\n\n　　* 链接数，即有多少文件名指向这个inode\n\n　　* 文件数据block的位置\n\n大量小文件的产生导致服务器inode占用过高，触发告警，如果inode继续增加可能会导致无法创建文件。\n\n\n****解决思路****\n\n将多个小文件合并为一个大文件处理，思路有很多：\n\n\n\n1. 可以合并为一个大的文本文件，每次记录内容的偏移量，读的时候根据起始位置和偏移量来获取内容。\n2. 仍保持单个文件的独立性，将多个文件合并为一个大的压缩文件。\n\n\n经过尝试发现，方案1可能会导致读写错误，具体表现为文本不但会增加也可能会删除，删除后要更新所有文件的起始值和偏移值，这反而加重了数据库的负担，而且偏移量一旦计算错误将会导致所有的文件内容读到的结果都是错误的。\n\n方案二保持了文件的独立性，只需记录文件路径即可，即便某个文件路径错误也不影响其他文件的读取，同理，删除也不会影响其他的文件读取，唯一要解决的便是并发环境即分布式环境的问题（同时读写文件），经测试和运行满足生产环境需求，且由于是一个压缩文件，inode占用率不会飙升，因此采用了方案二，并已生产部署。\n\n\n在此不详细讨论如何生成并读写压缩文件（将产生的多个文件放入压缩包中即可），着重说明如何保证同一时刻一个压缩文件不能被同时写入。\n\n****详细实现****    \n\t\n   Java 提供了文件锁FileLock类，利用这个类可以控制不同程序(JVM)对同一文件的并发访问，实现进程间文件同步操作。FileLock是java 1.4 版本后出现的一个类，它可以通过对一个可写文件(w)加锁，保证同时只有一个进程可以拿到文件的锁，这个进程从而可以对文件做访问；而其它拿不到锁的进程要么选择被挂起等待，要么选择去做一些其它的事情， 这样的机制保证了众进程可以顺序访问该文件。也可以看出，能够利用文件锁的这种性质，在一些场景下，虽然我们不需要操作某个文件， 但也可以通过 FileLock 来进行并发控制，保证进程的顺序执行，避免数据错误。 \n\n\n    \n\t1.保证获取到的文件对象唯一，每次写入动作时取到的文件是同一个\n\n\n\t\tprivate static synchronized File getInstance(String path) throws ZipException {\n\t\t\tif (map.containsKey(path)) {\n\t\t\t\treturn map.get(path);\n\t\t\t} else {\n\t\t\t\tCompressedFile compressedFile = new CompressedFile(path);\n\t\t\t\tzipMap.put(path, compressedFile);\n\t\t\t\treturn compressedFile;\n\t\t\t}\n\t\t}\n  2.使用FileLock锁确保同一时刻只有一个进程可以写文件,使用try with resource自动关闭文件流   \n\n        public static String writeZipContent(String filePath, String subPath, InputStream fis) throws Exception {\n\t\t\tFile zipFile = getInstance(filePath);\n\t\t\tsynchronized (zipFile) {\n\t\t\t\tFile f = new File(filePath);\n\t\t\t\tif (!f.getParentFile().exists()) {\n\t\t\t\t\tf.getParentFile().mkdirs();\n\t\t\t\t}\n\t\t\t\ttry (RandomAccessFile randomAccessFile = new RandomAccessFile(new File(zipFilePath + \".lock\"), \"rws\"); FileChannel channel = randomAccessFile.getChannel()) {\n\t\t\t\t\tFileLock fileLock;\n\t\t\t\t\tdo {\n\t\t\t\t\t\tfileLock = channel.lock();\n\t\t\t\t\t} while (fileLock == null || !fileLock.isValid());\n\t\n\t\t\t\t\t//do something\n\t\n\t\t\t\t\treturn \"path\";\n\t\t\t\t} catch (Exception ex) {\n\t\t\t\t\tlogger.error(ex.getMessage(), ex);\n\t\t\t\t\tthrow ex;\n\t\t\t\t}\n\t\t\t}\n    }\n\n\n\n****补充说明****   \n在java.io.RandomAccessFile类的open方法，提供了参数实现独占的方式打开文件：\n\n        RandomAccessFile raf = new RandomAccessFile(file, \"rws\");\n\n其中的“rws”参数，rw代表读取和写入，s代表了同步方式，也就是同步锁。\n\n\n\n\n\n\n\n​ \n​","source":"_posts/Java进程间文件锁.md","raw":"---\ntitle: Java进程间共享锁   \ndate: 2017-08-12 11:47:44   \ncategories: \"Java\"  \ntags: [lock file]    \ndescription: Java进程间共享锁\n\n---\n\n****背景****     \n业务系统运行时产生了大量的小文件（大小在10kb左右，包含一些表单的内容，rest接口的访问记录），考虑到将这些内容存储在数据库中对数据库压力较大，因此转为文件存储，但由于restful接口访问频繁，导致产生了大量的小文件，最终导致服务器inode占用率过高，触发告警\n\n\n****原因****    \n\n什么是inode?\ninode包含文件的元信息，具体来说有以下内容：\n\n　　* 文件的字节数\n\n　　* 文件拥有者的User ID\n\n　　* 文件的Group ID\n\n　　* 文件的读、写、执行权限\n\n　　* 文件的时间戳，共有三个：ctime指inode上一次变动的时间，mtime指文件内容上一次变动的时间，atime指文件上一次打开的时间。\n\n　　* 链接数，即有多少文件名指向这个inode\n\n　　* 文件数据block的位置\n\n大量小文件的产生导致服务器inode占用过高，触发告警，如果inode继续增加可能会导致无法创建文件。\n\n\n****解决思路****\n\n将多个小文件合并为一个大文件处理，思路有很多：\n\n\n\n1. 可以合并为一个大的文本文件，每次记录内容的偏移量，读的时候根据起始位置和偏移量来获取内容。\n2. 仍保持单个文件的独立性，将多个文件合并为一个大的压缩文件。\n\n\n经过尝试发现，方案1可能会导致读写错误，具体表现为文本不但会增加也可能会删除，删除后要更新所有文件的起始值和偏移值，这反而加重了数据库的负担，而且偏移量一旦计算错误将会导致所有的文件内容读到的结果都是错误的。\n\n方案二保持了文件的独立性，只需记录文件路径即可，即便某个文件路径错误也不影响其他文件的读取，同理，删除也不会影响其他的文件读取，唯一要解决的便是并发环境即分布式环境的问题（同时读写文件），经测试和运行满足生产环境需求，且由于是一个压缩文件，inode占用率不会飙升，因此采用了方案二，并已生产部署。\n\n\n在此不详细讨论如何生成并读写压缩文件（将产生的多个文件放入压缩包中即可），着重说明如何保证同一时刻一个压缩文件不能被同时写入。\n\n****详细实现****    \n\t\n   Java 提供了文件锁FileLock类，利用这个类可以控制不同程序(JVM)对同一文件的并发访问，实现进程间文件同步操作。FileLock是java 1.4 版本后出现的一个类，它可以通过对一个可写文件(w)加锁，保证同时只有一个进程可以拿到文件的锁，这个进程从而可以对文件做访问；而其它拿不到锁的进程要么选择被挂起等待，要么选择去做一些其它的事情， 这样的机制保证了众进程可以顺序访问该文件。也可以看出，能够利用文件锁的这种性质，在一些场景下，虽然我们不需要操作某个文件， 但也可以通过 FileLock 来进行并发控制，保证进程的顺序执行，避免数据错误。 \n\n\n    \n\t1.保证获取到的文件对象唯一，每次写入动作时取到的文件是同一个\n\n\n\t\tprivate static synchronized File getInstance(String path) throws ZipException {\n\t\t\tif (map.containsKey(path)) {\n\t\t\t\treturn map.get(path);\n\t\t\t} else {\n\t\t\t\tCompressedFile compressedFile = new CompressedFile(path);\n\t\t\t\tzipMap.put(path, compressedFile);\n\t\t\t\treturn compressedFile;\n\t\t\t}\n\t\t}\n  2.使用FileLock锁确保同一时刻只有一个进程可以写文件,使用try with resource自动关闭文件流   \n\n        public static String writeZipContent(String filePath, String subPath, InputStream fis) throws Exception {\n\t\t\tFile zipFile = getInstance(filePath);\n\t\t\tsynchronized (zipFile) {\n\t\t\t\tFile f = new File(filePath);\n\t\t\t\tif (!f.getParentFile().exists()) {\n\t\t\t\t\tf.getParentFile().mkdirs();\n\t\t\t\t}\n\t\t\t\ttry (RandomAccessFile randomAccessFile = new RandomAccessFile(new File(zipFilePath + \".lock\"), \"rws\"); FileChannel channel = randomAccessFile.getChannel()) {\n\t\t\t\t\tFileLock fileLock;\n\t\t\t\t\tdo {\n\t\t\t\t\t\tfileLock = channel.lock();\n\t\t\t\t\t} while (fileLock == null || !fileLock.isValid());\n\t\n\t\t\t\t\t//do something\n\t\n\t\t\t\t\treturn \"path\";\n\t\t\t\t} catch (Exception ex) {\n\t\t\t\t\tlogger.error(ex.getMessage(), ex);\n\t\t\t\t\tthrow ex;\n\t\t\t\t}\n\t\t\t}\n    }\n\n\n\n****补充说明****   \n在java.io.RandomAccessFile类的open方法，提供了参数实现独占的方式打开文件：\n\n        RandomAccessFile raf = new RandomAccessFile(file, \"rws\");\n\n其中的“rws”参数，rw代表读取和写入，s代表了同步方式，也就是同步锁。\n\n\n\n\n\n\n\n​ \n​","slug":"Java进程间文件锁","published":1,"updated":"2019-05-06T06:03:26.727Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cli5o2dqf0000czzkdml80vjp","content":"<p><strong><strong>背景</strong></strong><br>业务系统运行时产生了大量的小文件（大小在10kb左右，包含一些表单的内容，rest接口的访问记录），考虑到将这些内容存储在数据库中对数据库压力较大，因此转为文件存储，但由于restful接口访问频繁，导致产生了大量的小文件，最终导致服务器inode占用率过高，触发告警</p>\n<p><strong><strong>原因</strong></strong>    </p>\n<p>什么是inode?<br>inode包含文件的元信息，具体来说有以下内容：</p>\n<p>　　* 文件的字节数</p>\n<p>　　* 文件拥有者的User ID</p>\n<p>　　* 文件的Group ID</p>\n<p>　　* 文件的读、写、执行权限</p>\n<p>　　* 文件的时间戳，共有三个：ctime指inode上一次变动的时间，mtime指文件内容上一次变动的时间，atime指文件上一次打开的时间。</p>\n<p>　　* 链接数，即有多少文件名指向这个inode</p>\n<p>　　* 文件数据block的位置</p>\n<p>大量小文件的产生导致服务器inode占用过高，触发告警，如果inode继续增加可能会导致无法创建文件。</p>\n<p><strong><strong>解决思路</strong></strong></p>\n<p>将多个小文件合并为一个大文件处理，思路有很多：</p>\n<ol>\n<li>可以合并为一个大的文本文件，每次记录内容的偏移量，读的时候根据起始位置和偏移量来获取内容。</li>\n<li>仍保持单个文件的独立性，将多个文件合并为一个大的压缩文件。</li>\n</ol>\n<p>经过尝试发现，方案1可能会导致读写错误，具体表现为文本不但会增加也可能会删除，删除后要更新所有文件的起始值和偏移值，这反而加重了数据库的负担，而且偏移量一旦计算错误将会导致所有的文件内容读到的结果都是错误的。</p>\n<p>方案二保持了文件的独立性，只需记录文件路径即可，即便某个文件路径错误也不影响其他文件的读取，同理，删除也不会影响其他的文件读取，唯一要解决的便是并发环境即分布式环境的问题（同时读写文件），经测试和运行满足生产环境需求，且由于是一个压缩文件，inode占用率不会飙升，因此采用了方案二，并已生产部署。</p>\n<p>在此不详细讨论如何生成并读写压缩文件（将产生的多个文件放入压缩包中即可），着重说明如何保证同一时刻一个压缩文件不能被同时写入。</p>\n<p><strong><strong>详细实现</strong></strong>    </p>\n<p>   Java 提供了文件锁FileLock类，利用这个类可以控制不同程序(JVM)对同一文件的并发访问，实现进程间文件同步操作。FileLock是java 1.4 版本后出现的一个类，它可以通过对一个可写文件(w)加锁，保证同时只有一个进程可以拿到文件的锁，这个进程从而可以对文件做访问；而其它拿不到锁的进程要么选择被挂起等待，要么选择去做一些其它的事情， 这样的机制保证了众进程可以顺序访问该文件。也可以看出，能够利用文件锁的这种性质，在一些场景下，虽然我们不需要操作某个文件， 但也可以通过 FileLock 来进行并发控制，保证进程的顺序执行，避免数据错误。 </p>\n<pre><code>1.保证获取到的文件对象唯一，每次写入动作时取到的文件是同一个\n\n\n    private static synchronized File getInstance(String path) throws ZipException &#123;\n        if (map.containsKey(path)) &#123;\n            return map.get(path);\n        &#125; else &#123;\n            CompressedFile compressedFile = new CompressedFile(path);\n            zipMap.put(path, compressedFile);\n            return compressedFile;\n        &#125;\n    &#125;\n</code></pre>\n<p>  2.使用FileLock锁确保同一时刻只有一个进程可以写文件,使用try with resource自动关闭文件流   </p>\n<pre><code>    public static String writeZipContent(String filePath, String subPath, InputStream fis) throws Exception &#123;\n        File zipFile = getInstance(filePath);\n        synchronized (zipFile) &#123;\n            File f = new File(filePath);\n            if (!f.getParentFile().exists()) &#123;\n                f.getParentFile().mkdirs();\n            &#125;\n            try (RandomAccessFile randomAccessFile = new RandomAccessFile(new File(zipFilePath + &quot;.lock&quot;), &quot;rws&quot;); FileChannel channel = randomAccessFile.getChannel()) &#123;\n                FileLock fileLock;\n                do &#123;\n                    fileLock = channel.lock();\n                &#125; while (fileLock == null || !fileLock.isValid());\n\n                //do something\n\n                return &quot;path&quot;;\n            &#125; catch (Exception ex) &#123;\n                logger.error(ex.getMessage(), ex);\n                throw ex;\n            &#125;\n        &#125;\n&#125;\n</code></pre>\n<p><strong><strong>补充说明</strong></strong><br>在java.io.RandomAccessFile类的open方法，提供了参数实现独占的方式打开文件：</p>\n<pre><code>    RandomAccessFile raf = new RandomAccessFile(file, &quot;rws&quot;);\n</code></pre>\n<p>其中的“rws”参数，rw代表读取和写入，s代表了同步方式，也就是同步锁。</p>\n<p>​<br>​</p>\n","site":{"data":{}},"cover":false,"excerpt":"","more":"<p><strong><strong>背景</strong></strong><br>业务系统运行时产生了大量的小文件（大小在10kb左右，包含一些表单的内容，rest接口的访问记录），考虑到将这些内容存储在数据库中对数据库压力较大，因此转为文件存储，但由于restful接口访问频繁，导致产生了大量的小文件，最终导致服务器inode占用率过高，触发告警</p>\n<p><strong><strong>原因</strong></strong>    </p>\n<p>什么是inode?<br>inode包含文件的元信息，具体来说有以下内容：</p>\n<p>　　* 文件的字节数</p>\n<p>　　* 文件拥有者的User ID</p>\n<p>　　* 文件的Group ID</p>\n<p>　　* 文件的读、写、执行权限</p>\n<p>　　* 文件的时间戳，共有三个：ctime指inode上一次变动的时间，mtime指文件内容上一次变动的时间，atime指文件上一次打开的时间。</p>\n<p>　　* 链接数，即有多少文件名指向这个inode</p>\n<p>　　* 文件数据block的位置</p>\n<p>大量小文件的产生导致服务器inode占用过高，触发告警，如果inode继续增加可能会导致无法创建文件。</p>\n<p><strong><strong>解决思路</strong></strong></p>\n<p>将多个小文件合并为一个大文件处理，思路有很多：</p>\n<ol>\n<li>可以合并为一个大的文本文件，每次记录内容的偏移量，读的时候根据起始位置和偏移量来获取内容。</li>\n<li>仍保持单个文件的独立性，将多个文件合并为一个大的压缩文件。</li>\n</ol>\n<p>经过尝试发现，方案1可能会导致读写错误，具体表现为文本不但会增加也可能会删除，删除后要更新所有文件的起始值和偏移值，这反而加重了数据库的负担，而且偏移量一旦计算错误将会导致所有的文件内容读到的结果都是错误的。</p>\n<p>方案二保持了文件的独立性，只需记录文件路径即可，即便某个文件路径错误也不影响其他文件的读取，同理，删除也不会影响其他的文件读取，唯一要解决的便是并发环境即分布式环境的问题（同时读写文件），经测试和运行满足生产环境需求，且由于是一个压缩文件，inode占用率不会飙升，因此采用了方案二，并已生产部署。</p>\n<p>在此不详细讨论如何生成并读写压缩文件（将产生的多个文件放入压缩包中即可），着重说明如何保证同一时刻一个压缩文件不能被同时写入。</p>\n<p><strong><strong>详细实现</strong></strong>    </p>\n<p>   Java 提供了文件锁FileLock类，利用这个类可以控制不同程序(JVM)对同一文件的并发访问，实现进程间文件同步操作。FileLock是java 1.4 版本后出现的一个类，它可以通过对一个可写文件(w)加锁，保证同时只有一个进程可以拿到文件的锁，这个进程从而可以对文件做访问；而其它拿不到锁的进程要么选择被挂起等待，要么选择去做一些其它的事情， 这样的机制保证了众进程可以顺序访问该文件。也可以看出，能够利用文件锁的这种性质，在一些场景下，虽然我们不需要操作某个文件， 但也可以通过 FileLock 来进行并发控制，保证进程的顺序执行，避免数据错误。 </p>\n<pre><code>1.保证获取到的文件对象唯一，每次写入动作时取到的文件是同一个\n\n\n    private static synchronized File getInstance(String path) throws ZipException &#123;\n        if (map.containsKey(path)) &#123;\n            return map.get(path);\n        &#125; else &#123;\n            CompressedFile compressedFile = new CompressedFile(path);\n            zipMap.put(path, compressedFile);\n            return compressedFile;\n        &#125;\n    &#125;\n</code></pre>\n<p>  2.使用FileLock锁确保同一时刻只有一个进程可以写文件,使用try with resource自动关闭文件流   </p>\n<pre><code>    public static String writeZipContent(String filePath, String subPath, InputStream fis) throws Exception &#123;\n        File zipFile = getInstance(filePath);\n        synchronized (zipFile) &#123;\n            File f = new File(filePath);\n            if (!f.getParentFile().exists()) &#123;\n                f.getParentFile().mkdirs();\n            &#125;\n            try (RandomAccessFile randomAccessFile = new RandomAccessFile(new File(zipFilePath + &quot;.lock&quot;), &quot;rws&quot;); FileChannel channel = randomAccessFile.getChannel()) &#123;\n                FileLock fileLock;\n                do &#123;\n                    fileLock = channel.lock();\n                &#125; while (fileLock == null || !fileLock.isValid());\n\n                //do something\n\n                return &quot;path&quot;;\n            &#125; catch (Exception ex) &#123;\n                logger.error(ex.getMessage(), ex);\n                throw ex;\n            &#125;\n        &#125;\n&#125;\n</code></pre>\n<p><strong><strong>补充说明</strong></strong><br>在java.io.RandomAccessFile类的open方法，提供了参数实现独占的方式打开文件：</p>\n<pre><code>    RandomAccessFile raf = new RandomAccessFile(file, &quot;rws&quot;);\n</code></pre>\n<p>其中的“rws”参数，rw代表读取和写入，s代表了同步方式，也就是同步锁。</p>\n<p>​<br>​</p>\n"},{"title":"System.currentTimeMillis性能问题测试与处理","date":"2019-07-10T02:21:52.000Z","description":"Java_System.currentTimeMillis()性能问题测试与处理","_content":"\n****背景****     \n新项目打算做前后端分离，需要关注接口的响应时间，以便在出现问题时定位到是哪个接口，记得偶然看到获取多线程并发环境下该方法对性能可能会有影响，因此在项目前期做一个小测试。         \n\n\n\n\n****过程****    \n\nSystem.currentTimeMillis()在java中是最常用的获取系统时间的方法，它返回的是1970年1月1日0点到现在经过的毫秒数。    我们看以看到它在Java中的实现为：\n`public static native long currentTimeMillis();` \n\nnative关键字说明这个方法底层是由C语言实现的，我们稍后在讲，先做一个简单的测试。\n\n这次测试电脑配置为 Intel(R) Core(TM) i7-8550 16内存。测试很简单10000次for循环调用和多线程10000次调用，直接看结果：   \n单线程下10000次调用消耗时间为 6713300 ns    \n多线程下10000次调用消耗时间为 4968593100 ns  \n\n可以看到相差还是比较大的,在串行情况下这个api其实性能很好，但是在并发情况下回急剧下降，原因在于计时器在所有进程之间共享，并且其还一直在发生变化，当大量线程尝试同时去访问计时器的时候，就涉及到资源的竞争，于是也就出现并行效率远低于串行效率的现象了。所以在高并发场景下要慎重使用System.nanoTime()和System.currentTimeMillis()这两个API。  \n\n在搜索过程中我找到的一篇大家都比较信服的文章来解释为什么会这样，连接如下:   \n[http://pzemtsov.github.io/2017/07/23/the-slow-currenttimemillis.html](http://pzemtsov.github.io/2017/07/23/the-slow-currenttimemillis.html \"The slow currentTimeMillis()\")    \n\n​\n\n\n文章很长，讲的很详细，甚至从汇编语言的角度讲了为什么会这样，有几个比较重要的观点： \n\n>* 调用gettimeofday()需要从用户态切换到内核态；   \n>* gettimeofday()的表现受Linux系统的计时器（时钟源）影响，在HPET计时器下性能尤其差；   \n>* 系统只有一个全局时钟源，高并发或频繁访问会造成严重的争用\n\n\nHPET计时器问题在处理器层面已经解决\n处理器系列以不同方式增加时间戳计数器：\n\n> 对于奔腾M处理器（系列[06H]，型号[09H，0DH]）；对于奔腾4处理器，英特尔至强处理器（系列[0FH]，型号[00H，01H或02H]）；对于P6系列处理器：时间戳记计数器会随着每个内部处理器时钟周期的增加而增加。内部处理器时钟周期由当前内核时钟与总线时钟之比确定。英特尔®SpeedStep®技术过渡也可能会影响处理器时钟。\n> \n> 对于奔腾4处理器，英特尔至强处理器（系列[0FH]，型号[03H及更高版本]）；适用于Intel Core Solo和Intel Core Duo处理器（系列[06H]，型号[0EH]）；用于Intel Xeon处理器5100系列和Intel Core 2 Duo处理器（系列[06H]，型号[0FH]）；适用于Intel Core 2和Intel Xeon处理器（系列[06H]，DisplayModel [17H]）；对于Intel Atom处理器（系列[06H]，DisplayModel [1CH]）：时间戳记计数器以恒定速率递增。  \n\n****解决思路****\n\n网上的解决方法有很多：   \n1. 维护一个全局缓存，使用单线程调度器器按毫秒更新时间戳，用到了 `ScheduledThreadPoolExecutor`，代码如下    \n```\npublic class SystemClock {\n    private static final SystemClock MILLIS_CLOCK = new SystemClock(1);\n    private final long precision;\n    private final AtomicLong now;\n\n    private SystemClock(long precision) {\n        this.precision = precision;\n        now = new AtomicLong(System.currentTimeMillis());\n        scheduleClockUpdating();\n    }\n\n    public static SystemClock millisClock() {\n        return MILLIS_CLOCK;\n    }\n\n    private void scheduleClockUpdating() {\n        ScheduledExecutorService scheduler = Executors.newSingleThreadScheduledExecutor(runnable -> {\n            Thread thread = new Thread(runnable, \"system.clock\");\n            thread.setDaemon(true);\n            return thread;\n        });\n        scheduler.scheduleAtFixedRate(() -> now.set(System.currentTimeMillis()), precision, precision, TimeUnit.MILLISECONDS);\n    }\n\n    public long now() {\n        return now.get();\n    }\n\n```\n\n​ \n​","source":"_posts/System.currentTimeMillis性能问题测试与处理.md","raw":"---\ntitle: System.currentTimeMillis性能问题测试与处理   \ndate: 2019-07-10 10:21:52   \ncategories: \"Java\"  \ntags: [性能 时间戳]    \ndescription: Java_System.currentTimeMillis()性能问题测试与处理    \n\n---\n\n****背景****     \n新项目打算做前后端分离，需要关注接口的响应时间，以便在出现问题时定位到是哪个接口，记得偶然看到获取多线程并发环境下该方法对性能可能会有影响，因此在项目前期做一个小测试。         \n\n\n\n\n****过程****    \n\nSystem.currentTimeMillis()在java中是最常用的获取系统时间的方法，它返回的是1970年1月1日0点到现在经过的毫秒数。    我们看以看到它在Java中的实现为：\n`public static native long currentTimeMillis();` \n\nnative关键字说明这个方法底层是由C语言实现的，我们稍后在讲，先做一个简单的测试。\n\n这次测试电脑配置为 Intel(R) Core(TM) i7-8550 16内存。测试很简单10000次for循环调用和多线程10000次调用，直接看结果：   \n单线程下10000次调用消耗时间为 6713300 ns    \n多线程下10000次调用消耗时间为 4968593100 ns  \n\n可以看到相差还是比较大的,在串行情况下这个api其实性能很好，但是在并发情况下回急剧下降，原因在于计时器在所有进程之间共享，并且其还一直在发生变化，当大量线程尝试同时去访问计时器的时候，就涉及到资源的竞争，于是也就出现并行效率远低于串行效率的现象了。所以在高并发场景下要慎重使用System.nanoTime()和System.currentTimeMillis()这两个API。  \n\n在搜索过程中我找到的一篇大家都比较信服的文章来解释为什么会这样，连接如下:   \n[http://pzemtsov.github.io/2017/07/23/the-slow-currenttimemillis.html](http://pzemtsov.github.io/2017/07/23/the-slow-currenttimemillis.html \"The slow currentTimeMillis()\")    \n\n​\n\n\n文章很长，讲的很详细，甚至从汇编语言的角度讲了为什么会这样，有几个比较重要的观点： \n\n>* 调用gettimeofday()需要从用户态切换到内核态；   \n>* gettimeofday()的表现受Linux系统的计时器（时钟源）影响，在HPET计时器下性能尤其差；   \n>* 系统只有一个全局时钟源，高并发或频繁访问会造成严重的争用\n\n\nHPET计时器问题在处理器层面已经解决\n处理器系列以不同方式增加时间戳计数器：\n\n> 对于奔腾M处理器（系列[06H]，型号[09H，0DH]）；对于奔腾4处理器，英特尔至强处理器（系列[0FH]，型号[00H，01H或02H]）；对于P6系列处理器：时间戳记计数器会随着每个内部处理器时钟周期的增加而增加。内部处理器时钟周期由当前内核时钟与总线时钟之比确定。英特尔®SpeedStep®技术过渡也可能会影响处理器时钟。\n> \n> 对于奔腾4处理器，英特尔至强处理器（系列[0FH]，型号[03H及更高版本]）；适用于Intel Core Solo和Intel Core Duo处理器（系列[06H]，型号[0EH]）；用于Intel Xeon处理器5100系列和Intel Core 2 Duo处理器（系列[06H]，型号[0FH]）；适用于Intel Core 2和Intel Xeon处理器（系列[06H]，DisplayModel [17H]）；对于Intel Atom处理器（系列[06H]，DisplayModel [1CH]）：时间戳记计数器以恒定速率递增。  \n\n****解决思路****\n\n网上的解决方法有很多：   \n1. 维护一个全局缓存，使用单线程调度器器按毫秒更新时间戳，用到了 `ScheduledThreadPoolExecutor`，代码如下    \n```\npublic class SystemClock {\n    private static final SystemClock MILLIS_CLOCK = new SystemClock(1);\n    private final long precision;\n    private final AtomicLong now;\n\n    private SystemClock(long precision) {\n        this.precision = precision;\n        now = new AtomicLong(System.currentTimeMillis());\n        scheduleClockUpdating();\n    }\n\n    public static SystemClock millisClock() {\n        return MILLIS_CLOCK;\n    }\n\n    private void scheduleClockUpdating() {\n        ScheduledExecutorService scheduler = Executors.newSingleThreadScheduledExecutor(runnable -> {\n            Thread thread = new Thread(runnable, \"system.clock\");\n            thread.setDaemon(true);\n            return thread;\n        });\n        scheduler.scheduleAtFixedRate(() -> now.set(System.currentTimeMillis()), precision, precision, TimeUnit.MILLISECONDS);\n    }\n\n    public long now() {\n        return now.get();\n    }\n\n```\n\n​ \n​","slug":"System.currentTimeMillis性能问题测试与处理","published":1,"updated":"2023-05-27T06:56:56.010Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cli5o2dql0001czzkckq1fhn1","content":"<p><strong><strong>背景</strong></strong><br>新项目打算做前后端分离，需要关注接口的响应时间，以便在出现问题时定位到是哪个接口，记得偶然看到获取多线程并发环境下该方法对性能可能会有影响，因此在项目前期做一个小测试。         </p>\n<p><strong><strong>过程</strong></strong>    </p>\n<p>System.currentTimeMillis()在java中是最常用的获取系统时间的方法，它返回的是1970年1月1日0点到现在经过的毫秒数。    我们看以看到它在Java中的实现为：<br><code>public static native long currentTimeMillis();</code> </p>\n<p>native关键字说明这个方法底层是由C语言实现的，我们稍后在讲，先做一个简单的测试。</p>\n<p>这次测试电脑配置为 Intel(R) Core(TM) i7-8550 16内存。测试很简单10000次for循环调用和多线程10000次调用，直接看结果：<br>单线程下10000次调用消耗时间为 6713300 ns<br>多线程下10000次调用消耗时间为 4968593100 ns  </p>\n<p>可以看到相差还是比较大的,在串行情况下这个api其实性能很好，但是在并发情况下回急剧下降，原因在于计时器在所有进程之间共享，并且其还一直在发生变化，当大量线程尝试同时去访问计时器的时候，就涉及到资源的竞争，于是也就出现并行效率远低于串行效率的现象了。所以在高并发场景下要慎重使用System.nanoTime()和System.currentTimeMillis()这两个API。  </p>\n<p>在搜索过程中我找到的一篇大家都比较信服的文章来解释为什么会这样，连接如下:<br><a href=\"http://pzemtsov.github.io/2017/07/23/the-slow-currenttimemillis.html\" title=\"The slow currentTimeMillis()\">http://pzemtsov.github.io/2017/07/23/the-slow-currenttimemillis.html</a>    </p>\n<p>​</p>\n<p>文章很长，讲的很详细，甚至从汇编语言的角度讲了为什么会这样，有几个比较重要的观点： </p>\n<blockquote>\n<ul>\n<li>调用gettimeofday()需要从用户态切换到内核态；   </li>\n<li>gettimeofday()的表现受Linux系统的计时器（时钟源）影响，在HPET计时器下性能尤其差；   </li>\n<li>系统只有一个全局时钟源，高并发或频繁访问会造成严重的争用</li>\n</ul>\n</blockquote>\n<p>HPET计时器问题在处理器层面已经解决<br>处理器系列以不同方式增加时间戳计数器：</p>\n<blockquote>\n<p>对于奔腾M处理器（系列[06H]，型号[09H，0DH]）；对于奔腾4处理器，英特尔至强处理器（系列[0FH]，型号[00H，01H或02H]）；对于P6系列处理器：时间戳记计数器会随着每个内部处理器时钟周期的增加而增加。内部处理器时钟周期由当前内核时钟与总线时钟之比确定。英特尔®SpeedStep®技术过渡也可能会影响处理器时钟。</p>\n<p>对于奔腾4处理器，英特尔至强处理器（系列[0FH]，型号[03H及更高版本]）；适用于Intel Core Solo和Intel Core Duo处理器（系列[06H]，型号[0EH]）；用于Intel Xeon处理器5100系列和Intel Core 2 Duo处理器（系列[06H]，型号[0FH]）；适用于Intel Core 2和Intel Xeon处理器（系列[06H]，DisplayModel [17H]）；对于Intel Atom处理器（系列[06H]，DisplayModel [1CH]）：时间戳记计数器以恒定速率递增。  </p>\n</blockquote>\n<p><strong><strong>解决思路</strong></strong></p>\n<p>网上的解决方法有很多：   </p>\n<ol>\n<li>维护一个全局缓存，使用单线程调度器器按毫秒更新时间戳，用到了 <code>ScheduledThreadPoolExecutor</code>，代码如下    <figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public class SystemClock &#123;</span><br><span class=\"line\">    private static final SystemClock MILLIS_CLOCK = new SystemClock(1);</span><br><span class=\"line\">    private final long precision;</span><br><span class=\"line\">    private final AtomicLong now;</span><br><span class=\"line\"></span><br><span class=\"line\">    private SystemClock(long precision) &#123;</span><br><span class=\"line\">        this.precision = precision;</span><br><span class=\"line\">        now = new AtomicLong(System.currentTimeMillis());</span><br><span class=\"line\">        scheduleClockUpdating();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    public static SystemClock millisClock() &#123;</span><br><span class=\"line\">        return MILLIS_CLOCK;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    private void scheduleClockUpdating() &#123;</span><br><span class=\"line\">        ScheduledExecutorService scheduler = Executors.newSingleThreadScheduledExecutor(runnable -&gt; &#123;</span><br><span class=\"line\">            Thread thread = new Thread(runnable, &quot;system.clock&quot;);</span><br><span class=\"line\">            thread.setDaemon(true);</span><br><span class=\"line\">            return thread;</span><br><span class=\"line\">        &#125;);</span><br><span class=\"line\">        scheduler.scheduleAtFixedRate(() -&gt; now.set(System.currentTimeMillis()), precision, precision, TimeUnit.MILLISECONDS);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    public long now() &#123;</span><br><span class=\"line\">        return now.get();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure></li>\n</ol>\n<p>​<br>​</p>\n","site":{"data":{}},"cover":false,"excerpt":"","more":"<p><strong><strong>背景</strong></strong><br>新项目打算做前后端分离，需要关注接口的响应时间，以便在出现问题时定位到是哪个接口，记得偶然看到获取多线程并发环境下该方法对性能可能会有影响，因此在项目前期做一个小测试。         </p>\n<p><strong><strong>过程</strong></strong>    </p>\n<p>System.currentTimeMillis()在java中是最常用的获取系统时间的方法，它返回的是1970年1月1日0点到现在经过的毫秒数。    我们看以看到它在Java中的实现为：<br><code>public static native long currentTimeMillis();</code> </p>\n<p>native关键字说明这个方法底层是由C语言实现的，我们稍后在讲，先做一个简单的测试。</p>\n<p>这次测试电脑配置为 Intel(R) Core(TM) i7-8550 16内存。测试很简单10000次for循环调用和多线程10000次调用，直接看结果：<br>单线程下10000次调用消耗时间为 6713300 ns<br>多线程下10000次调用消耗时间为 4968593100 ns  </p>\n<p>可以看到相差还是比较大的,在串行情况下这个api其实性能很好，但是在并发情况下回急剧下降，原因在于计时器在所有进程之间共享，并且其还一直在发生变化，当大量线程尝试同时去访问计时器的时候，就涉及到资源的竞争，于是也就出现并行效率远低于串行效率的现象了。所以在高并发场景下要慎重使用System.nanoTime()和System.currentTimeMillis()这两个API。  </p>\n<p>在搜索过程中我找到的一篇大家都比较信服的文章来解释为什么会这样，连接如下:<br><a href=\"http://pzemtsov.github.io/2017/07/23/the-slow-currenttimemillis.html\" title=\"The slow currentTimeMillis()\">http://pzemtsov.github.io/2017/07/23/the-slow-currenttimemillis.html</a>    </p>\n<p>​</p>\n<p>文章很长，讲的很详细，甚至从汇编语言的角度讲了为什么会这样，有几个比较重要的观点： </p>\n<blockquote>\n<ul>\n<li>调用gettimeofday()需要从用户态切换到内核态；   </li>\n<li>gettimeofday()的表现受Linux系统的计时器（时钟源）影响，在HPET计时器下性能尤其差；   </li>\n<li>系统只有一个全局时钟源，高并发或频繁访问会造成严重的争用</li>\n</ul>\n</blockquote>\n<p>HPET计时器问题在处理器层面已经解决<br>处理器系列以不同方式增加时间戳计数器：</p>\n<blockquote>\n<p>对于奔腾M处理器（系列[06H]，型号[09H，0DH]）；对于奔腾4处理器，英特尔至强处理器（系列[0FH]，型号[00H，01H或02H]）；对于P6系列处理器：时间戳记计数器会随着每个内部处理器时钟周期的增加而增加。内部处理器时钟周期由当前内核时钟与总线时钟之比确定。英特尔®SpeedStep®技术过渡也可能会影响处理器时钟。</p>\n<p>对于奔腾4处理器，英特尔至强处理器（系列[0FH]，型号[03H及更高版本]）；适用于Intel Core Solo和Intel Core Duo处理器（系列[06H]，型号[0EH]）；用于Intel Xeon处理器5100系列和Intel Core 2 Duo处理器（系列[06H]，型号[0FH]）；适用于Intel Core 2和Intel Xeon处理器（系列[06H]，DisplayModel [17H]）；对于Intel Atom处理器（系列[06H]，DisplayModel [1CH]）：时间戳记计数器以恒定速率递增。  </p>\n</blockquote>\n<p><strong><strong>解决思路</strong></strong></p>\n<p>网上的解决方法有很多：   </p>\n<ol>\n<li>维护一个全局缓存，使用单线程调度器器按毫秒更新时间戳，用到了 <code>ScheduledThreadPoolExecutor</code>，代码如下    <figure class=\"highlight plaintext\"><table><tr><td class=\"gutter\"><pre><span class=\"line\">1</span><br><span class=\"line\">2</span><br><span class=\"line\">3</span><br><span class=\"line\">4</span><br><span class=\"line\">5</span><br><span class=\"line\">6</span><br><span class=\"line\">7</span><br><span class=\"line\">8</span><br><span class=\"line\">9</span><br><span class=\"line\">10</span><br><span class=\"line\">11</span><br><span class=\"line\">12</span><br><span class=\"line\">13</span><br><span class=\"line\">14</span><br><span class=\"line\">15</span><br><span class=\"line\">16</span><br><span class=\"line\">17</span><br><span class=\"line\">18</span><br><span class=\"line\">19</span><br><span class=\"line\">20</span><br><span class=\"line\">21</span><br><span class=\"line\">22</span><br><span class=\"line\">23</span><br><span class=\"line\">24</span><br><span class=\"line\">25</span><br><span class=\"line\">26</span><br><span class=\"line\">27</span><br><span class=\"line\">28</span><br></pre></td><td class=\"code\"><pre><span class=\"line\">public class SystemClock &#123;</span><br><span class=\"line\">    private static final SystemClock MILLIS_CLOCK = new SystemClock(1);</span><br><span class=\"line\">    private final long precision;</span><br><span class=\"line\">    private final AtomicLong now;</span><br><span class=\"line\"></span><br><span class=\"line\">    private SystemClock(long precision) &#123;</span><br><span class=\"line\">        this.precision = precision;</span><br><span class=\"line\">        now = new AtomicLong(System.currentTimeMillis());</span><br><span class=\"line\">        scheduleClockUpdating();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    public static SystemClock millisClock() &#123;</span><br><span class=\"line\">        return MILLIS_CLOCK;</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    private void scheduleClockUpdating() &#123;</span><br><span class=\"line\">        ScheduledExecutorService scheduler = Executors.newSingleThreadScheduledExecutor(runnable -&gt; &#123;</span><br><span class=\"line\">            Thread thread = new Thread(runnable, &quot;system.clock&quot;);</span><br><span class=\"line\">            thread.setDaemon(true);</span><br><span class=\"line\">            return thread;</span><br><span class=\"line\">        &#125;);</span><br><span class=\"line\">        scheduler.scheduleAtFixedRate(() -&gt; now.set(System.currentTimeMillis()), precision, precision, TimeUnit.MILLISECONDS);</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br><span class=\"line\">    public long now() &#123;</span><br><span class=\"line\">        return now.get();</span><br><span class=\"line\">    &#125;</span><br><span class=\"line\"></span><br></pre></td></tr></table></figure></li>\n</ol>\n<p>​<br>​</p>\n"},{"title":"Spring事务基本概念","date":"2016-09-03T15:47:44.000Z","description":"Spring事务基本概念","_content":"\n****事务隔离级别****     \n\n\n  隔离级别是指若干个并发的事务之间的隔离程度。TransactionDefinition 接口中定义了五个表示隔离级别的常量：     \n\n- TransactionDefinition.ISOLATION_DEFAULT：这是默认值，表示使用底层数据库的默认隔离级别。对大部分数据库而言，通常这值就是TransactionDefinition.ISOLATION_READ_COMMITTED。    \n- TransactionDefinition.ISOLATION_READ_UNCOMMITTED：该隔离级别表示一个事务可以读取另一个事务修改但还没有提交的数据。该级别不能防止脏读，不可重复读和幻读，因此很少使用该隔离级别。比如PostgreSQL实际上并没有此级别。    \n- TransactionDefinition.ISOLATION_READ_COMMITTED：该隔离级别表示一个事务只能读取另一个事务已经提交的数据。该级别可以防止脏读，这也是大多数情况下的推荐值。    \n- TransactionDefinition.ISOLATION_REPEATABLE_READ：该隔离级别表示一个事务在整个过程中可以多次重复执行某个查询，并且每次返回的记录都相同。该级别可以防止脏读和不可重复读。       \n- TransactionDefinition.ISOLATION_SERIALIZABLE：所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。但是这将严重影响程序的性能。通常情况下也不会用到该级别。\n \n****事务传播行为**** \n   \n所谓事务的传播行为是指，如果在开始当前事务之前，一个事务上下文已经存在，此时有若干选项可以指定一个事务性方法的执行行为。在TransactionDefinition定义中包括了如下几个表示传播行为的常量：\n\n- TransactionDefinition.PROPAGATION_REQUIRED：如果当前存在事务，则加入该事务；如果当前没有事务，则创建一个新的事务。这是默认值。\n- TransactionDefinition.PROPAGATION_REQUIRES_NEW：创建一个新的事务，如果当前存在事务，则把当前事务挂起。\n- TransactionDefinition.PROPAGATION_SUPPORTS：如果当前存在事务，则加入该事务；如果当前没有事务，则以非事务的方式继续运行。\n- TransactionDefinition.PROPAGATION_NOT_SUPPORTED：以非事务方式运行，如果当前存在事务，则把当前事务挂起。\n- TransactionDefinition.PROPAGATION_NEVER：以非事务方式运行，如果当前存在事务，则抛出异常。\n- TransactionDefinition.PROPAGATION_MANDATORY：如果当前存在事务，则加入该事务；如果当前没有事务，则抛出异常。\n- TransactionDefinition.PROPAGATION_NESTED：如果当前存在事务，则创建一个事务作为当前事务的嵌套事务来运行；如果当前没有事务，则该取值等价于TransactionDefinition.PROPAGATION_REQUIRED。\n\n****事务超时**** \n   \n所谓事务超时，就是指一个事务所允许执行的最长时间，如果超过该时间限制但事务还没有完成，则自动回滚事务。在 TransactionDefinition 中以 int 的值来表示超时时间，其单位是秒。\n默认设置为底层事务系统的超时值，如果底层数据库事务系统没有设置超时值，那么就是none，没有超时限制。\n\n    \n****spring事务回滚规则****   \n指示spring事务管理器回滚一个事务的推荐方法是在当前事务的上下文内抛出异常。spring事务管理器会捕捉任何未处理的异常，然后依据规则决定是否回滚抛出异常的事务。默认配置下，spring只有在抛出的异常为运行时unchecked异常时才回滚该事务，也就是抛出的异常为RuntimeException的子类(Errors也会导致事务回滚)，而抛出checked异常则不会导致事务回滚。可以明确的配置在抛出那些异常时回滚事务，包括checked异常。也可以明确定义那些异常抛出时不回滚事务。还可以编程性的通过setRollbackOnly()方法来指示一个事务必须回滚，在调用完setRollbackOnly()后你所能执行的唯一操作就是回滚。\n\n\n​****事务只读属性****   \n只读事务用于客户代码只读但不修改数据的情形，只读事务用于特定情景下的优化，比如使用Hibernate的时候。默认为读写事务。    \n“只读事务”并不是一个强制选项，它只是一个“暗示”，提示数据库驱动程序和数据库系统，这个事务并不包含更改数据的操作，那么JDBC驱动程序和数据库就有可能根据这种情况对该事务进行一些特定的优化，比方说不安排相应的数据库锁，以减轻事务对数据库的压力，毕竟事务也是要消耗数据库的资源的。但是你非要在“只读事务”里面修改数据，也并非不可以，只不过对于数据一致性的保护不像“读写事务”那样保险而已。因此，“只读事务”仅仅是一个性能优化的推荐配置而已，并非强制你要这样做不可\n\n****mybatis事务****   \nMyBatis自动参与到spring事务管理中，无需额外配置，只要org.mybatis.spring.SqlSessionFactoryBean引用的数据源与DataSourceTransactionManager引用的数据源一致即可，否则事务管理会不起作用。  \n\n\n****@Transactional注解****\n\n@Transactional属性 \n\n|属性\t|类型\t|描述|   \n| :-----| :----- | :----- |\n|value\t|String\t|可选的限定描述符,指定使用的事务管理器|   \n|propagation\t|enum: Propagation\t|可选的事务传播行为设置|   \n|isolation\t|enum: Isolation\t|可选的事务隔离级别设置|   \n|readOnly\t|boolean\t|读写或只读事务，默认读写|   \n|timeout\t|int (in seconds granularity)\t|事务超时时间设置|   \n|rollbackFor\t|Class对象数组，必须继承自Throwable\t|导致事务回滚的异常类数组|   \n|rollbackForClassName\t|类名数组，必须继承自Throwable\t|导致事务回滚的异常类名字数组|   \n|noRollbackFor\t|Class对象数组，必须继承自Throwable\t|不会导致事务回滚的异常类数组|   \n|noRollbackForClassName\t|类名数组，必须继承自Throwable\t|不会导致事务回滚的异常类名字数组|   \n​","source":"_posts/Spring事务基本概念.md","raw":"---\ntitle: Spring事务基本概念  \ndate: 2016-09-03 23:47:44   \ncategories: \"spring\"  \ntags: [spring transaction]    \ndescription: Spring事务基本概念\n\n---\n\n****事务隔离级别****     \n\n\n  隔离级别是指若干个并发的事务之间的隔离程度。TransactionDefinition 接口中定义了五个表示隔离级别的常量：     \n\n- TransactionDefinition.ISOLATION_DEFAULT：这是默认值，表示使用底层数据库的默认隔离级别。对大部分数据库而言，通常这值就是TransactionDefinition.ISOLATION_READ_COMMITTED。    \n- TransactionDefinition.ISOLATION_READ_UNCOMMITTED：该隔离级别表示一个事务可以读取另一个事务修改但还没有提交的数据。该级别不能防止脏读，不可重复读和幻读，因此很少使用该隔离级别。比如PostgreSQL实际上并没有此级别。    \n- TransactionDefinition.ISOLATION_READ_COMMITTED：该隔离级别表示一个事务只能读取另一个事务已经提交的数据。该级别可以防止脏读，这也是大多数情况下的推荐值。    \n- TransactionDefinition.ISOLATION_REPEATABLE_READ：该隔离级别表示一个事务在整个过程中可以多次重复执行某个查询，并且每次返回的记录都相同。该级别可以防止脏读和不可重复读。       \n- TransactionDefinition.ISOLATION_SERIALIZABLE：所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。但是这将严重影响程序的性能。通常情况下也不会用到该级别。\n \n****事务传播行为**** \n   \n所谓事务的传播行为是指，如果在开始当前事务之前，一个事务上下文已经存在，此时有若干选项可以指定一个事务性方法的执行行为。在TransactionDefinition定义中包括了如下几个表示传播行为的常量：\n\n- TransactionDefinition.PROPAGATION_REQUIRED：如果当前存在事务，则加入该事务；如果当前没有事务，则创建一个新的事务。这是默认值。\n- TransactionDefinition.PROPAGATION_REQUIRES_NEW：创建一个新的事务，如果当前存在事务，则把当前事务挂起。\n- TransactionDefinition.PROPAGATION_SUPPORTS：如果当前存在事务，则加入该事务；如果当前没有事务，则以非事务的方式继续运行。\n- TransactionDefinition.PROPAGATION_NOT_SUPPORTED：以非事务方式运行，如果当前存在事务，则把当前事务挂起。\n- TransactionDefinition.PROPAGATION_NEVER：以非事务方式运行，如果当前存在事务，则抛出异常。\n- TransactionDefinition.PROPAGATION_MANDATORY：如果当前存在事务，则加入该事务；如果当前没有事务，则抛出异常。\n- TransactionDefinition.PROPAGATION_NESTED：如果当前存在事务，则创建一个事务作为当前事务的嵌套事务来运行；如果当前没有事务，则该取值等价于TransactionDefinition.PROPAGATION_REQUIRED。\n\n****事务超时**** \n   \n所谓事务超时，就是指一个事务所允许执行的最长时间，如果超过该时间限制但事务还没有完成，则自动回滚事务。在 TransactionDefinition 中以 int 的值来表示超时时间，其单位是秒。\n默认设置为底层事务系统的超时值，如果底层数据库事务系统没有设置超时值，那么就是none，没有超时限制。\n\n    \n****spring事务回滚规则****   \n指示spring事务管理器回滚一个事务的推荐方法是在当前事务的上下文内抛出异常。spring事务管理器会捕捉任何未处理的异常，然后依据规则决定是否回滚抛出异常的事务。默认配置下，spring只有在抛出的异常为运行时unchecked异常时才回滚该事务，也就是抛出的异常为RuntimeException的子类(Errors也会导致事务回滚)，而抛出checked异常则不会导致事务回滚。可以明确的配置在抛出那些异常时回滚事务，包括checked异常。也可以明确定义那些异常抛出时不回滚事务。还可以编程性的通过setRollbackOnly()方法来指示一个事务必须回滚，在调用完setRollbackOnly()后你所能执行的唯一操作就是回滚。\n\n\n​****事务只读属性****   \n只读事务用于客户代码只读但不修改数据的情形，只读事务用于特定情景下的优化，比如使用Hibernate的时候。默认为读写事务。    \n“只读事务”并不是一个强制选项，它只是一个“暗示”，提示数据库驱动程序和数据库系统，这个事务并不包含更改数据的操作，那么JDBC驱动程序和数据库就有可能根据这种情况对该事务进行一些特定的优化，比方说不安排相应的数据库锁，以减轻事务对数据库的压力，毕竟事务也是要消耗数据库的资源的。但是你非要在“只读事务”里面修改数据，也并非不可以，只不过对于数据一致性的保护不像“读写事务”那样保险而已。因此，“只读事务”仅仅是一个性能优化的推荐配置而已，并非强制你要这样做不可\n\n****mybatis事务****   \nMyBatis自动参与到spring事务管理中，无需额外配置，只要org.mybatis.spring.SqlSessionFactoryBean引用的数据源与DataSourceTransactionManager引用的数据源一致即可，否则事务管理会不起作用。  \n\n\n****@Transactional注解****\n\n@Transactional属性 \n\n|属性\t|类型\t|描述|   \n| :-----| :----- | :----- |\n|value\t|String\t|可选的限定描述符,指定使用的事务管理器|   \n|propagation\t|enum: Propagation\t|可选的事务传播行为设置|   \n|isolation\t|enum: Isolation\t|可选的事务隔离级别设置|   \n|readOnly\t|boolean\t|读写或只读事务，默认读写|   \n|timeout\t|int (in seconds granularity)\t|事务超时时间设置|   \n|rollbackFor\t|Class对象数组，必须继承自Throwable\t|导致事务回滚的异常类数组|   \n|rollbackForClassName\t|类名数组，必须继承自Throwable\t|导致事务回滚的异常类名字数组|   \n|noRollbackFor\t|Class对象数组，必须继承自Throwable\t|不会导致事务回滚的异常类数组|   \n|noRollbackForClassName\t|类名数组，必须继承自Throwable\t|不会导致事务回滚的异常类名字数组|   \n​","slug":"Spring事务基本概念","published":1,"updated":"2019-09-30T02:37:29.052Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cli5o2dqr0004czzk31si2iet","content":"<p><strong><strong>事务隔离级别</strong></strong>     </p>\n<p>  隔离级别是指若干个并发的事务之间的隔离程度。TransactionDefinition 接口中定义了五个表示隔离级别的常量：     </p>\n<ul>\n<li>TransactionDefinition.ISOLATION_DEFAULT：这是默认值，表示使用底层数据库的默认隔离级别。对大部分数据库而言，通常这值就是TransactionDefinition.ISOLATION_READ_COMMITTED。    </li>\n<li>TransactionDefinition.ISOLATION_READ_UNCOMMITTED：该隔离级别表示一个事务可以读取另一个事务修改但还没有提交的数据。该级别不能防止脏读，不可重复读和幻读，因此很少使用该隔离级别。比如PostgreSQL实际上并没有此级别。    </li>\n<li>TransactionDefinition.ISOLATION_READ_COMMITTED：该隔离级别表示一个事务只能读取另一个事务已经提交的数据。该级别可以防止脏读，这也是大多数情况下的推荐值。    </li>\n<li>TransactionDefinition.ISOLATION_REPEATABLE_READ：该隔离级别表示一个事务在整个过程中可以多次重复执行某个查询，并且每次返回的记录都相同。该级别可以防止脏读和不可重复读。       </li>\n<li>TransactionDefinition.ISOLATION_SERIALIZABLE：所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。但是这将严重影响程序的性能。通常情况下也不会用到该级别。</li>\n</ul>\n<p><strong><strong>事务传播行为</strong></strong> </p>\n<p>所谓事务的传播行为是指，如果在开始当前事务之前，一个事务上下文已经存在，此时有若干选项可以指定一个事务性方法的执行行为。在TransactionDefinition定义中包括了如下几个表示传播行为的常量：</p>\n<ul>\n<li>TransactionDefinition.PROPAGATION_REQUIRED：如果当前存在事务，则加入该事务；如果当前没有事务，则创建一个新的事务。这是默认值。</li>\n<li>TransactionDefinition.PROPAGATION_REQUIRES_NEW：创建一个新的事务，如果当前存在事务，则把当前事务挂起。</li>\n<li>TransactionDefinition.PROPAGATION_SUPPORTS：如果当前存在事务，则加入该事务；如果当前没有事务，则以非事务的方式继续运行。</li>\n<li>TransactionDefinition.PROPAGATION_NOT_SUPPORTED：以非事务方式运行，如果当前存在事务，则把当前事务挂起。</li>\n<li>TransactionDefinition.PROPAGATION_NEVER：以非事务方式运行，如果当前存在事务，则抛出异常。</li>\n<li>TransactionDefinition.PROPAGATION_MANDATORY：如果当前存在事务，则加入该事务；如果当前没有事务，则抛出异常。</li>\n<li>TransactionDefinition.PROPAGATION_NESTED：如果当前存在事务，则创建一个事务作为当前事务的嵌套事务来运行；如果当前没有事务，则该取值等价于TransactionDefinition.PROPAGATION_REQUIRED。</li>\n</ul>\n<p><strong><strong>事务超时</strong></strong> </p>\n<p>所谓事务超时，就是指一个事务所允许执行的最长时间，如果超过该时间限制但事务还没有完成，则自动回滚事务。在 TransactionDefinition 中以 int 的值来表示超时时间，其单位是秒。<br>默认设置为底层事务系统的超时值，如果底层数据库事务系统没有设置超时值，那么就是none，没有超时限制。</p>\n<p><strong><strong>spring事务回滚规则</strong></strong><br>指示spring事务管理器回滚一个事务的推荐方法是在当前事务的上下文内抛出异常。spring事务管理器会捕捉任何未处理的异常，然后依据规则决定是否回滚抛出异常的事务。默认配置下，spring只有在抛出的异常为运行时unchecked异常时才回滚该事务，也就是抛出的异常为RuntimeException的子类(Errors也会导致事务回滚)，而抛出checked异常则不会导致事务回滚。可以明确的配置在抛出那些异常时回滚事务，包括checked异常。也可以明确定义那些异常抛出时不回滚事务。还可以编程性的通过setRollbackOnly()方法来指示一个事务必须回滚，在调用完setRollbackOnly()后你所能执行的唯一操作就是回滚。</p>\n<p>​<strong><strong>事务只读属性</strong></strong><br>只读事务用于客户代码只读但不修改数据的情形，只读事务用于特定情景下的优化，比如使用Hibernate的时候。默认为读写事务。<br>“只读事务”并不是一个强制选项，它只是一个“暗示”，提示数据库驱动程序和数据库系统，这个事务并不包含更改数据的操作，那么JDBC驱动程序和数据库就有可能根据这种情况对该事务进行一些特定的优化，比方说不安排相应的数据库锁，以减轻事务对数据库的压力，毕竟事务也是要消耗数据库的资源的。但是你非要在“只读事务”里面修改数据，也并非不可以，只不过对于数据一致性的保护不像“读写事务”那样保险而已。因此，“只读事务”仅仅是一个性能优化的推荐配置而已，并非强制你要这样做不可</p>\n<p><strong><strong>mybatis事务</strong></strong><br>MyBatis自动参与到spring事务管理中，无需额外配置，只要org.mybatis.spring.SqlSessionFactoryBean引用的数据源与DataSourceTransactionManager引用的数据源一致即可，否则事务管理会不起作用。  </p>\n<p><strong><strong>@Transactional注解</strong></strong></p>\n<p>@Transactional属性 </p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">属性</th>\n<th align=\"left\">类型</th>\n<th align=\"left\">描述</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">value</td>\n<td align=\"left\">String</td>\n<td align=\"left\">可选的限定描述符,指定使用的事务管理器</td>\n</tr>\n<tr>\n<td align=\"left\">propagation</td>\n<td align=\"left\">enum: Propagation</td>\n<td align=\"left\">可选的事务传播行为设置</td>\n</tr>\n<tr>\n<td align=\"left\">isolation</td>\n<td align=\"left\">enum: Isolation</td>\n<td align=\"left\">可选的事务隔离级别设置</td>\n</tr>\n<tr>\n<td align=\"left\">readOnly</td>\n<td align=\"left\">boolean</td>\n<td align=\"left\">读写或只读事务，默认读写</td>\n</tr>\n<tr>\n<td align=\"left\">timeout</td>\n<td align=\"left\">int (in seconds granularity)</td>\n<td align=\"left\">事务超时时间设置</td>\n</tr>\n<tr>\n<td align=\"left\">rollbackFor</td>\n<td align=\"left\">Class对象数组，必须继承自Throwable</td>\n<td align=\"left\">导致事务回滚的异常类数组</td>\n</tr>\n<tr>\n<td align=\"left\">rollbackForClassName</td>\n<td align=\"left\">类名数组，必须继承自Throwable</td>\n<td align=\"left\">导致事务回滚的异常类名字数组</td>\n</tr>\n<tr>\n<td align=\"left\">noRollbackFor</td>\n<td align=\"left\">Class对象数组，必须继承自Throwable</td>\n<td align=\"left\">不会导致事务回滚的异常类数组</td>\n</tr>\n<tr>\n<td align=\"left\">noRollbackForClassName</td>\n<td align=\"left\">类名数组，必须继承自Throwable</td>\n<td align=\"left\">不会导致事务回滚的异常类名字数组</td>\n</tr>\n<tr>\n<td align=\"left\">​</td>\n<td align=\"left\"></td>\n<td align=\"left\"></td>\n</tr>\n</tbody></table>\n","site":{"data":{}},"cover":false,"excerpt":"","more":"<p><strong><strong>事务隔离级别</strong></strong>     </p>\n<p>  隔离级别是指若干个并发的事务之间的隔离程度。TransactionDefinition 接口中定义了五个表示隔离级别的常量：     </p>\n<ul>\n<li>TransactionDefinition.ISOLATION_DEFAULT：这是默认值，表示使用底层数据库的默认隔离级别。对大部分数据库而言，通常这值就是TransactionDefinition.ISOLATION_READ_COMMITTED。    </li>\n<li>TransactionDefinition.ISOLATION_READ_UNCOMMITTED：该隔离级别表示一个事务可以读取另一个事务修改但还没有提交的数据。该级别不能防止脏读，不可重复读和幻读，因此很少使用该隔离级别。比如PostgreSQL实际上并没有此级别。    </li>\n<li>TransactionDefinition.ISOLATION_READ_COMMITTED：该隔离级别表示一个事务只能读取另一个事务已经提交的数据。该级别可以防止脏读，这也是大多数情况下的推荐值。    </li>\n<li>TransactionDefinition.ISOLATION_REPEATABLE_READ：该隔离级别表示一个事务在整个过程中可以多次重复执行某个查询，并且每次返回的记录都相同。该级别可以防止脏读和不可重复读。       </li>\n<li>TransactionDefinition.ISOLATION_SERIALIZABLE：所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。但是这将严重影响程序的性能。通常情况下也不会用到该级别。</li>\n</ul>\n<p><strong><strong>事务传播行为</strong></strong> </p>\n<p>所谓事务的传播行为是指，如果在开始当前事务之前，一个事务上下文已经存在，此时有若干选项可以指定一个事务性方法的执行行为。在TransactionDefinition定义中包括了如下几个表示传播行为的常量：</p>\n<ul>\n<li>TransactionDefinition.PROPAGATION_REQUIRED：如果当前存在事务，则加入该事务；如果当前没有事务，则创建一个新的事务。这是默认值。</li>\n<li>TransactionDefinition.PROPAGATION_REQUIRES_NEW：创建一个新的事务，如果当前存在事务，则把当前事务挂起。</li>\n<li>TransactionDefinition.PROPAGATION_SUPPORTS：如果当前存在事务，则加入该事务；如果当前没有事务，则以非事务的方式继续运行。</li>\n<li>TransactionDefinition.PROPAGATION_NOT_SUPPORTED：以非事务方式运行，如果当前存在事务，则把当前事务挂起。</li>\n<li>TransactionDefinition.PROPAGATION_NEVER：以非事务方式运行，如果当前存在事务，则抛出异常。</li>\n<li>TransactionDefinition.PROPAGATION_MANDATORY：如果当前存在事务，则加入该事务；如果当前没有事务，则抛出异常。</li>\n<li>TransactionDefinition.PROPAGATION_NESTED：如果当前存在事务，则创建一个事务作为当前事务的嵌套事务来运行；如果当前没有事务，则该取值等价于TransactionDefinition.PROPAGATION_REQUIRED。</li>\n</ul>\n<p><strong><strong>事务超时</strong></strong> </p>\n<p>所谓事务超时，就是指一个事务所允许执行的最长时间，如果超过该时间限制但事务还没有完成，则自动回滚事务。在 TransactionDefinition 中以 int 的值来表示超时时间，其单位是秒。<br>默认设置为底层事务系统的超时值，如果底层数据库事务系统没有设置超时值，那么就是none，没有超时限制。</p>\n<p><strong><strong>spring事务回滚规则</strong></strong><br>指示spring事务管理器回滚一个事务的推荐方法是在当前事务的上下文内抛出异常。spring事务管理器会捕捉任何未处理的异常，然后依据规则决定是否回滚抛出异常的事务。默认配置下，spring只有在抛出的异常为运行时unchecked异常时才回滚该事务，也就是抛出的异常为RuntimeException的子类(Errors也会导致事务回滚)，而抛出checked异常则不会导致事务回滚。可以明确的配置在抛出那些异常时回滚事务，包括checked异常。也可以明确定义那些异常抛出时不回滚事务。还可以编程性的通过setRollbackOnly()方法来指示一个事务必须回滚，在调用完setRollbackOnly()后你所能执行的唯一操作就是回滚。</p>\n<p>​<strong><strong>事务只读属性</strong></strong><br>只读事务用于客户代码只读但不修改数据的情形，只读事务用于特定情景下的优化，比如使用Hibernate的时候。默认为读写事务。<br>“只读事务”并不是一个强制选项，它只是一个“暗示”，提示数据库驱动程序和数据库系统，这个事务并不包含更改数据的操作，那么JDBC驱动程序和数据库就有可能根据这种情况对该事务进行一些特定的优化，比方说不安排相应的数据库锁，以减轻事务对数据库的压力，毕竟事务也是要消耗数据库的资源的。但是你非要在“只读事务”里面修改数据，也并非不可以，只不过对于数据一致性的保护不像“读写事务”那样保险而已。因此，“只读事务”仅仅是一个性能优化的推荐配置而已，并非强制你要这样做不可</p>\n<p><strong><strong>mybatis事务</strong></strong><br>MyBatis自动参与到spring事务管理中，无需额外配置，只要org.mybatis.spring.SqlSessionFactoryBean引用的数据源与DataSourceTransactionManager引用的数据源一致即可，否则事务管理会不起作用。  </p>\n<p><strong><strong>@Transactional注解</strong></strong></p>\n<p>@Transactional属性 </p>\n<table>\n<thead>\n<tr>\n<th align=\"left\">属性</th>\n<th align=\"left\">类型</th>\n<th align=\"left\">描述</th>\n</tr>\n</thead>\n<tbody><tr>\n<td align=\"left\">value</td>\n<td align=\"left\">String</td>\n<td align=\"left\">可选的限定描述符,指定使用的事务管理器</td>\n</tr>\n<tr>\n<td align=\"left\">propagation</td>\n<td align=\"left\">enum: Propagation</td>\n<td align=\"left\">可选的事务传播行为设置</td>\n</tr>\n<tr>\n<td align=\"left\">isolation</td>\n<td align=\"left\">enum: Isolation</td>\n<td align=\"left\">可选的事务隔离级别设置</td>\n</tr>\n<tr>\n<td align=\"left\">readOnly</td>\n<td align=\"left\">boolean</td>\n<td align=\"left\">读写或只读事务，默认读写</td>\n</tr>\n<tr>\n<td align=\"left\">timeout</td>\n<td align=\"left\">int (in seconds granularity)</td>\n<td align=\"left\">事务超时时间设置</td>\n</tr>\n<tr>\n<td align=\"left\">rollbackFor</td>\n<td align=\"left\">Class对象数组，必须继承自Throwable</td>\n<td align=\"left\">导致事务回滚的异常类数组</td>\n</tr>\n<tr>\n<td align=\"left\">rollbackForClassName</td>\n<td align=\"left\">类名数组，必须继承自Throwable</td>\n<td align=\"left\">导致事务回滚的异常类名字数组</td>\n</tr>\n<tr>\n<td align=\"left\">noRollbackFor</td>\n<td align=\"left\">Class对象数组，必须继承自Throwable</td>\n<td align=\"left\">不会导致事务回滚的异常类数组</td>\n</tr>\n<tr>\n<td align=\"left\">noRollbackForClassName</td>\n<td align=\"left\">类名数组，必须继承自Throwable</td>\n<td align=\"left\">不会导致事务回滚的异常类名字数组</td>\n</tr>\n<tr>\n<td align=\"left\">​</td>\n<td align=\"left\"></td>\n<td align=\"left\"></td>\n</tr>\n</tbody></table>\n"},{"title":"websoket构建站内通知","date":"2019-04-10T12:05:32.000Z","description":"websoket构建站内通知","_content":"\n****背景****     \n传统的报表或者统计展示都是统计一段时间的数据，然后由网页发起接口调用进而展示数据，对于一些不需要实时展示的内容来说是可以，但对于一些需要即时看到并处理的信息则可能会延后。\n\n\n****原因****    \n\n网页端定时获取数据可能会造成信息的滞后，对于一些需要立即处理的事件可能会延误。\n\n\n****解决思路****\n\n不再使用前端定时调用接口的形式获取数据，使用websocket来即时从后台推送数据到前端。\n\n\n\n1. 使用主流长连接框架netty实现websocket功能。\n2. 使用jJavaEE7支持的@ServerEndpoint注解实现websocket。\n\n\n若使用netty实现，性能会较好，但需要新启端口用于netty的连接与端口监听（默认为8888），使用注解实现则可以共用web 8080端口，减轻了实施的工作量，企业用户也无需新开防火墙申请变更，因此使用@ServerEndpoint实现websocket达到实时发送信息的目的。（在之前的章节提到了如何使用netty来做心跳检测以及信息传递）\n\n\n****详细实现****    \n\t    \n\n\t\n1.在类上使用注解@ServerEndpoint，并指定编解码类，因为实际场景的数据不只是字符串那么简单，一般都是json对象。还需实现  onOpen onClose onError三个方法，用于处理打开，关闭，异常时的处理场景。 同时onMessage方法用户处理客户端向服务端发送信息时的处理逻辑。       \n \n   \n    import net.sf.json.JSONObject;\n    import org.slf4j.Logger;\n    import org.slf4j.LoggerFactory;\n    \n    import java.io.IOException;\n    \n    import javax.websocket.*;\n    import javax.websocket.server.ServerEndpoint;\n    \n    @ServerEndpoint(value = \"/websocket\", encoders = {MessageEncoder.class })\n    public class SystemRemindSocketServer {\n\n\t    private Logger logger = LoggerFactory.getLogger(SystemRemindSocketServer.class);\n\t\n\t    @OnMessage\n\t    public void onMessage(String message, Session session) throws IOException, InterruptedException, EncodeException {\n\t        //暂时无需处理网页端发送的信息\n\t        //test\n\t        //        for(int i = 0;i<5;i++){\n\t        //            JSONObject data = new JSONObject();\n\t        //            data.put(\"icon\",\"ts-component\");\n\t        //            data.put(\"taskId\",1386);\n\t        //            data.put(\"time\",\"2019-03-26 14:31:30\");\n\t        //            data.put(\"title\",\"当时明月在,曾照彩云归.\");\n\t        //            data.put(\"link\",\"/\");\n\t        //            data.put(\"template\",\"balantflow.systemremind.message.flow\");\n\t        //            session.getBasicRemote().sendText(data.toString());\n\t        //        }\n\t    }\n\t\n\t    @OnOpen\n\t    public void onOpen(Session session) {\n\t        String userId = session.getUserPrincipal().getName();\n\t        String sessionId = session.getId();\n\t        SystemRemindMessageManager.sessionMap.put(userId, session);\n\t\n\t    }\n\t\n\t    @OnClose\n\t    public void onClose(Session session) {\n\t        String userId = session.getUserPrincipal().getName();\n\t        SystemRemindMessageManager.sessionMap.remove(userId);\n\t    }\n\t\n\t    @OnError\n\t    public void onError(Throwable t) {\n\t        logger.error(\"web socket error ,\" + t.getMessage());\n\t    }\n    }\n\n\n  2.编解码类的实现，直接用jsonobject格式化即可   \n\n    public class MessageEncoder implements Encoder.Text<SystemRemindMessageVo> {\n\n\t    @Override\n\t    public void destroy() {\n\t\n\t\n\t    }\n\t\n\t    @Override\n\t    public void init(EndpointConfig arg0) {\n\t\n\t\n\t    }\n\t\n\t    @Override\n\t    public String encode(SystemRemindMessageVo messagepojo) throws EncodeException {\n\t        try {\n\t            return JSONObject.fromObject(messagepojo).toString();\n\t        } catch (Exception e) {\n\t            return null;\n\t        }\n\t    }\n\n    }\n\n3.前端建立连接与数据处理    \n\n\t$(function(){\n        //启动时建立连接\n\t\tvar notifyws = new WebSocket(\"ws://localhost:8080/balantflow/websocket\");\n\t\tnotifyws.onopen = function(evt) { \n\t\t\t //console.log(evt);\n\t\t};\n\n        //后台发送消息时的处理逻辑\n\t\tnotifyws.onmessage = function(evt) {\n\t\t\tvar newnotify = [];\n\t\t\tnewnotify.push(JSON.parse(evt.data));\n\t\t\tupdateInstant(newnotify);\n\t\t};\n\n        //断开后重新连接\n\t\tnotifyws.onclose = function(evt) {\n\t\t\tnotifyws = new WebSocket(\"ws://localhost:8080/balantflow/websocket\");\n\t\t};\n    })\n\n****补充说明****   \n部署环境时要注意修改websocket配置，因为在用户机器上连接localhost:8080是没用的，要改为具体的websocket地址\n        \n\n\n\n\n\n\n\n​ \n​","source":"_posts/websocket构建站内通知.md","raw":"---\ntitle: websoket构建站内通知   \ndate: 2019-04-10 20:05:32   \ncategories: \"websocket\"  \ntags: [websocket]    \ndescription: websoket构建站内通知\n\n---\n\n****背景****     \n传统的报表或者统计展示都是统计一段时间的数据，然后由网页发起接口调用进而展示数据，对于一些不需要实时展示的内容来说是可以，但对于一些需要即时看到并处理的信息则可能会延后。\n\n\n****原因****    \n\n网页端定时获取数据可能会造成信息的滞后，对于一些需要立即处理的事件可能会延误。\n\n\n****解决思路****\n\n不再使用前端定时调用接口的形式获取数据，使用websocket来即时从后台推送数据到前端。\n\n\n\n1. 使用主流长连接框架netty实现websocket功能。\n2. 使用jJavaEE7支持的@ServerEndpoint注解实现websocket。\n\n\n若使用netty实现，性能会较好，但需要新启端口用于netty的连接与端口监听（默认为8888），使用注解实现则可以共用web 8080端口，减轻了实施的工作量，企业用户也无需新开防火墙申请变更，因此使用@ServerEndpoint实现websocket达到实时发送信息的目的。（在之前的章节提到了如何使用netty来做心跳检测以及信息传递）\n\n\n****详细实现****    \n\t    \n\n\t\n1.在类上使用注解@ServerEndpoint，并指定编解码类，因为实际场景的数据不只是字符串那么简单，一般都是json对象。还需实现  onOpen onClose onError三个方法，用于处理打开，关闭，异常时的处理场景。 同时onMessage方法用户处理客户端向服务端发送信息时的处理逻辑。       \n \n   \n    import net.sf.json.JSONObject;\n    import org.slf4j.Logger;\n    import org.slf4j.LoggerFactory;\n    \n    import java.io.IOException;\n    \n    import javax.websocket.*;\n    import javax.websocket.server.ServerEndpoint;\n    \n    @ServerEndpoint(value = \"/websocket\", encoders = {MessageEncoder.class })\n    public class SystemRemindSocketServer {\n\n\t    private Logger logger = LoggerFactory.getLogger(SystemRemindSocketServer.class);\n\t\n\t    @OnMessage\n\t    public void onMessage(String message, Session session) throws IOException, InterruptedException, EncodeException {\n\t        //暂时无需处理网页端发送的信息\n\t        //test\n\t        //        for(int i = 0;i<5;i++){\n\t        //            JSONObject data = new JSONObject();\n\t        //            data.put(\"icon\",\"ts-component\");\n\t        //            data.put(\"taskId\",1386);\n\t        //            data.put(\"time\",\"2019-03-26 14:31:30\");\n\t        //            data.put(\"title\",\"当时明月在,曾照彩云归.\");\n\t        //            data.put(\"link\",\"/\");\n\t        //            data.put(\"template\",\"balantflow.systemremind.message.flow\");\n\t        //            session.getBasicRemote().sendText(data.toString());\n\t        //        }\n\t    }\n\t\n\t    @OnOpen\n\t    public void onOpen(Session session) {\n\t        String userId = session.getUserPrincipal().getName();\n\t        String sessionId = session.getId();\n\t        SystemRemindMessageManager.sessionMap.put(userId, session);\n\t\n\t    }\n\t\n\t    @OnClose\n\t    public void onClose(Session session) {\n\t        String userId = session.getUserPrincipal().getName();\n\t        SystemRemindMessageManager.sessionMap.remove(userId);\n\t    }\n\t\n\t    @OnError\n\t    public void onError(Throwable t) {\n\t        logger.error(\"web socket error ,\" + t.getMessage());\n\t    }\n    }\n\n\n  2.编解码类的实现，直接用jsonobject格式化即可   \n\n    public class MessageEncoder implements Encoder.Text<SystemRemindMessageVo> {\n\n\t    @Override\n\t    public void destroy() {\n\t\n\t\n\t    }\n\t\n\t    @Override\n\t    public void init(EndpointConfig arg0) {\n\t\n\t\n\t    }\n\t\n\t    @Override\n\t    public String encode(SystemRemindMessageVo messagepojo) throws EncodeException {\n\t        try {\n\t            return JSONObject.fromObject(messagepojo).toString();\n\t        } catch (Exception e) {\n\t            return null;\n\t        }\n\t    }\n\n    }\n\n3.前端建立连接与数据处理    \n\n\t$(function(){\n        //启动时建立连接\n\t\tvar notifyws = new WebSocket(\"ws://localhost:8080/balantflow/websocket\");\n\t\tnotifyws.onopen = function(evt) { \n\t\t\t //console.log(evt);\n\t\t};\n\n        //后台发送消息时的处理逻辑\n\t\tnotifyws.onmessage = function(evt) {\n\t\t\tvar newnotify = [];\n\t\t\tnewnotify.push(JSON.parse(evt.data));\n\t\t\tupdateInstant(newnotify);\n\t\t};\n\n        //断开后重新连接\n\t\tnotifyws.onclose = function(evt) {\n\t\t\tnotifyws = new WebSocket(\"ws://localhost:8080/balantflow/websocket\");\n\t\t};\n    })\n\n****补充说明****   \n部署环境时要注意修改websocket配置，因为在用户机器上连接localhost:8080是没用的，要改为具体的websocket地址\n        \n\n\n\n\n\n\n\n​ \n​","slug":"websocket构建站内通知","published":1,"updated":"2019-05-06T06:42:00.022Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cli5o2dqs0005czzkdltx85at","content":"<p><strong><strong>背景</strong></strong><br>传统的报表或者统计展示都是统计一段时间的数据，然后由网页发起接口调用进而展示数据，对于一些不需要实时展示的内容来说是可以，但对于一些需要即时看到并处理的信息则可能会延后。</p>\n<p><strong><strong>原因</strong></strong>    </p>\n<p>网页端定时获取数据可能会造成信息的滞后，对于一些需要立即处理的事件可能会延误。</p>\n<p><strong><strong>解决思路</strong></strong></p>\n<p>不再使用前端定时调用接口的形式获取数据，使用websocket来即时从后台推送数据到前端。</p>\n<ol>\n<li>使用主流长连接框架netty实现websocket功能。</li>\n<li>使用jJavaEE7支持的@ServerEndpoint注解实现websocket。</li>\n</ol>\n<p>若使用netty实现，性能会较好，但需要新启端口用于netty的连接与端口监听（默认为8888），使用注解实现则可以共用web 8080端口，减轻了实施的工作量，企业用户也无需新开防火墙申请变更，因此使用@ServerEndpoint实现websocket达到实时发送信息的目的。（在之前的章节提到了如何使用netty来做心跳检测以及信息传递）</p>\n<p><strong><strong>详细实现</strong></strong>    </p>\n<p>1.在类上使用注解@ServerEndpoint，并指定编解码类，因为实际场景的数据不只是字符串那么简单，一般都是json对象。还需实现  onOpen onClose onError三个方法，用于处理打开，关闭，异常时的处理场景。 同时onMessage方法用户处理客户端向服务端发送信息时的处理逻辑。       </p>\n<pre><code>import net.sf.json.JSONObject;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport java.io.IOException;\n\nimport javax.websocket.*;\nimport javax.websocket.server.ServerEndpoint;\n\n@ServerEndpoint(value = &quot;/websocket&quot;, encoders = &#123;MessageEncoder.class &#125;)\npublic class SystemRemindSocketServer &#123;\n\n    private Logger logger = LoggerFactory.getLogger(SystemRemindSocketServer.class);\n\n    @OnMessage\n    public void onMessage(String message, Session session) throws IOException, InterruptedException, EncodeException &#123;\n        //暂时无需处理网页端发送的信息\n        //test\n        //        for(int i = 0;i&lt;5;i++)&#123;\n        //            JSONObject data = new JSONObject();\n        //            data.put(&quot;icon&quot;,&quot;ts-component&quot;);\n        //            data.put(&quot;taskId&quot;,1386);\n        //            data.put(&quot;time&quot;,&quot;2019-03-26 14:31:30&quot;);\n        //            data.put(&quot;title&quot;,&quot;当时明月在,曾照彩云归.&quot;);\n        //            data.put(&quot;link&quot;,&quot;/&quot;);\n        //            data.put(&quot;template&quot;,&quot;balantflow.systemremind.message.flow&quot;);\n        //            session.getBasicRemote().sendText(data.toString());\n        //        &#125;\n    &#125;\n\n    @OnOpen\n    public void onOpen(Session session) &#123;\n        String userId = session.getUserPrincipal().getName();\n        String sessionId = session.getId();\n        SystemRemindMessageManager.sessionMap.put(userId, session);\n\n    &#125;\n\n    @OnClose\n    public void onClose(Session session) &#123;\n        String userId = session.getUserPrincipal().getName();\n        SystemRemindMessageManager.sessionMap.remove(userId);\n    &#125;\n\n    @OnError\n    public void onError(Throwable t) &#123;\n        logger.error(&quot;web socket error ,&quot; + t.getMessage());\n    &#125;\n&#125;\n</code></pre>\n<p>  2.编解码类的实现，直接用jsonobject格式化即可   </p>\n<pre><code>public class MessageEncoder implements Encoder.Text&lt;SystemRemindMessageVo&gt; &#123;\n\n    @Override\n    public void destroy() &#123;\n\n\n    &#125;\n\n    @Override\n    public void init(EndpointConfig arg0) &#123;\n\n\n    &#125;\n\n    @Override\n    public String encode(SystemRemindMessageVo messagepojo) throws EncodeException &#123;\n        try &#123;\n            return JSONObject.fromObject(messagepojo).toString();\n        &#125; catch (Exception e) &#123;\n            return null;\n        &#125;\n    &#125;\n\n&#125;\n</code></pre>\n<p>3.前端建立连接与数据处理    </p>\n<pre><code>$(function()&#123;\n    //启动时建立连接\n    var notifyws = new WebSocket(&quot;ws://localhost:8080/balantflow/websocket&quot;);\n    notifyws.onopen = function(evt) &#123; \n         //console.log(evt);\n    &#125;;\n\n    //后台发送消息时的处理逻辑\n    notifyws.onmessage = function(evt) &#123;\n        var newnotify = [];\n        newnotify.push(JSON.parse(evt.data));\n        updateInstant(newnotify);\n    &#125;;\n\n    //断开后重新连接\n    notifyws.onclose = function(evt) &#123;\n        notifyws = new WebSocket(&quot;ws://localhost:8080/balantflow/websocket&quot;);\n    &#125;;\n&#125;)\n</code></pre>\n<p><strong><strong>补充说明</strong></strong><br>部署环境时要注意修改websocket配置，因为在用户机器上连接localhost:8080是没用的，要改为具体的websocket地址</p>\n<p>​<br>​</p>\n","site":{"data":{}},"cover":false,"excerpt":"","more":"<p><strong><strong>背景</strong></strong><br>传统的报表或者统计展示都是统计一段时间的数据，然后由网页发起接口调用进而展示数据，对于一些不需要实时展示的内容来说是可以，但对于一些需要即时看到并处理的信息则可能会延后。</p>\n<p><strong><strong>原因</strong></strong>    </p>\n<p>网页端定时获取数据可能会造成信息的滞后，对于一些需要立即处理的事件可能会延误。</p>\n<p><strong><strong>解决思路</strong></strong></p>\n<p>不再使用前端定时调用接口的形式获取数据，使用websocket来即时从后台推送数据到前端。</p>\n<ol>\n<li>使用主流长连接框架netty实现websocket功能。</li>\n<li>使用jJavaEE7支持的@ServerEndpoint注解实现websocket。</li>\n</ol>\n<p>若使用netty实现，性能会较好，但需要新启端口用于netty的连接与端口监听（默认为8888），使用注解实现则可以共用web 8080端口，减轻了实施的工作量，企业用户也无需新开防火墙申请变更，因此使用@ServerEndpoint实现websocket达到实时发送信息的目的。（在之前的章节提到了如何使用netty来做心跳检测以及信息传递）</p>\n<p><strong><strong>详细实现</strong></strong>    </p>\n<p>1.在类上使用注解@ServerEndpoint，并指定编解码类，因为实际场景的数据不只是字符串那么简单，一般都是json对象。还需实现  onOpen onClose onError三个方法，用于处理打开，关闭，异常时的处理场景。 同时onMessage方法用户处理客户端向服务端发送信息时的处理逻辑。       </p>\n<pre><code>import net.sf.json.JSONObject;\nimport org.slf4j.Logger;\nimport org.slf4j.LoggerFactory;\n\nimport java.io.IOException;\n\nimport javax.websocket.*;\nimport javax.websocket.server.ServerEndpoint;\n\n@ServerEndpoint(value = &quot;/websocket&quot;, encoders = &#123;MessageEncoder.class &#125;)\npublic class SystemRemindSocketServer &#123;\n\n    private Logger logger = LoggerFactory.getLogger(SystemRemindSocketServer.class);\n\n    @OnMessage\n    public void onMessage(String message, Session session) throws IOException, InterruptedException, EncodeException &#123;\n        //暂时无需处理网页端发送的信息\n        //test\n        //        for(int i = 0;i&lt;5;i++)&#123;\n        //            JSONObject data = new JSONObject();\n        //            data.put(&quot;icon&quot;,&quot;ts-component&quot;);\n        //            data.put(&quot;taskId&quot;,1386);\n        //            data.put(&quot;time&quot;,&quot;2019-03-26 14:31:30&quot;);\n        //            data.put(&quot;title&quot;,&quot;当时明月在,曾照彩云归.&quot;);\n        //            data.put(&quot;link&quot;,&quot;/&quot;);\n        //            data.put(&quot;template&quot;,&quot;balantflow.systemremind.message.flow&quot;);\n        //            session.getBasicRemote().sendText(data.toString());\n        //        &#125;\n    &#125;\n\n    @OnOpen\n    public void onOpen(Session session) &#123;\n        String userId = session.getUserPrincipal().getName();\n        String sessionId = session.getId();\n        SystemRemindMessageManager.sessionMap.put(userId, session);\n\n    &#125;\n\n    @OnClose\n    public void onClose(Session session) &#123;\n        String userId = session.getUserPrincipal().getName();\n        SystemRemindMessageManager.sessionMap.remove(userId);\n    &#125;\n\n    @OnError\n    public void onError(Throwable t) &#123;\n        logger.error(&quot;web socket error ,&quot; + t.getMessage());\n    &#125;\n&#125;\n</code></pre>\n<p>  2.编解码类的实现，直接用jsonobject格式化即可   </p>\n<pre><code>public class MessageEncoder implements Encoder.Text&lt;SystemRemindMessageVo&gt; &#123;\n\n    @Override\n    public void destroy() &#123;\n\n\n    &#125;\n\n    @Override\n    public void init(EndpointConfig arg0) &#123;\n\n\n    &#125;\n\n    @Override\n    public String encode(SystemRemindMessageVo messagepojo) throws EncodeException &#123;\n        try &#123;\n            return JSONObject.fromObject(messagepojo).toString();\n        &#125; catch (Exception e) &#123;\n            return null;\n        &#125;\n    &#125;\n\n&#125;\n</code></pre>\n<p>3.前端建立连接与数据处理    </p>\n<pre><code>$(function()&#123;\n    //启动时建立连接\n    var notifyws = new WebSocket(&quot;ws://localhost:8080/balantflow/websocket&quot;);\n    notifyws.onopen = function(evt) &#123; \n         //console.log(evt);\n    &#125;;\n\n    //后台发送消息时的处理逻辑\n    notifyws.onmessage = function(evt) &#123;\n        var newnotify = [];\n        newnotify.push(JSON.parse(evt.data));\n        updateInstant(newnotify);\n    &#125;;\n\n    //断开后重新连接\n    notifyws.onclose = function(evt) &#123;\n        notifyws = new WebSocket(&quot;ws://localhost:8080/balantflow/websocket&quot;);\n    &#125;;\n&#125;)\n</code></pre>\n<p><strong><strong>补充说明</strong></strong><br>部署环境时要注意修改websocket配置，因为在用户机器上连接localhost:8080是没用的，要改为具体的websocket地址</p>\n<p>​<br>​</p>\n"},{"title":"什么是好的设计","date":"2019-03-19T16:00:00.000Z","description":"什么是好的设计","_content":"\n好设计是能影响用户非理性，改变情绪最终影响他的行为。\n设计的高境界在于只影响不明说，用户不需要领会设计意图，但情绪已经被感染了，在情绪的作用下，按照设计者预定的轨迹，自然做出选择。","source":"_posts/什么是好的设计.md","raw":"---\ntitle: 什么是好的设计  \ndate: 2019-03-20 \ncategories: \"design\"  \ntags: [ design view]    \ndescription: 什么是好的设计\n\n---\n\n好设计是能影响用户非理性，改变情绪最终影响他的行为。\n设计的高境界在于只影响不明说，用户不需要领会设计意图，但情绪已经被感染了，在情绪的作用下，按照设计者预定的轨迹，自然做出选择。","slug":"什么是好的设计","published":1,"updated":"2023-05-27T07:06:54.116Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cli5o2dqt0006czzk2g2r69sz","content":"<p>好设计是能影响用户非理性，改变情绪最终影响他的行为。<br>设计的高境界在于只影响不明说，用户不需要领会设计意图，但情绪已经被感染了，在情绪的作用下，按照设计者预定的轨迹，自然做出选择。</p>\n","site":{"data":{}},"cover":false,"excerpt":"","more":"<p>好设计是能影响用户非理性，改变情绪最终影响他的行为。<br>设计的高境界在于只影响不明说，用户不需要领会设计意图，但情绪已经被感染了，在情绪的作用下，按照设计者预定的轨迹，自然做出选择。</p>\n"},{"title":"使用RateLimiter限制访问频率","date":"2019-06-12T09:23:44.000Z","description":"使用RateLimiter限制访问频率","_content":"\n****背景****     \n通过查看接口访问日志发现某个注册接口请求量突然暴涨，并引发连锁反应，导致整个系统变慢。系统已经对接口做了做了权限认证，包含token认证，时效认证，禁用启用等功能，但没有对访问频率做限制，除了需要优化处理逻辑外，也要加上对接口的访问频率限制，以防止非预期的请求对系统压力过大而引起的系统瘫痪，当流量过大时，可以采取拒绝或者引流等机制。\n\n\n****原因及方案分析****    \n\n常用的限流算法有两种：漏桶算法和令牌桶算法。\n\n漏桶(Leaky Bucket)算法思路很简单,水(请求)先进入到漏桶里,漏桶以一定的速度出水(接口有响应速率),当水流入速度过大会直接溢出(访问频率超过接口响应速率),然后就拒绝请求,可以看出漏桶算法能强行限制数据的传输速率。但是漏桶的漏出速率是固定的参数,所以,即使网络中不存在资源冲突(没有发生拥塞),漏桶算法也不能使流突发(burst)到端口速率.因此,漏桶算法对于存在突发特性的流量来说缺乏效率.\n![](https://i.imgur.com/doxSzsv.png)    \n令牌桶算法(Token Bucket)和 Leaky Bucket 效果一样但方向相反的算法,更加容易理解.随着时间流逝,系统会按恒定1/QPS时间间隔(如果QPS=100,则间隔是10ms)往桶里加入Token(想象和漏洞漏水相反,有个水龙头在不断的加水),如果桶已经满了就不再加了.新请求来临时,会各自拿走一个Token,如果没有Token可拿了就阻塞或者拒绝服务.令牌桶的另外一个好处是可以方便的改变速度. 一旦需要提高速率,则按需提高放入桶中的令牌的速率. 一般会定时(比如100毫秒)往桶中增加一定数量的令牌, 有些变种算法则实时的计算应该增加的令牌的数量.\n\n![](https://i.imgur.com/WCQ32wH.png)\n****解决思路****\n\n使用令牌桶算法限制接口访问频率sss。\n\n****详细实现****    \n\t\n   Google开源工具包Guava提供了限流工具类RateLimiter，该类基于令牌桶算法来完成限流，非常易于使用。在项目中用了以下实现 \n\n   1.频率是可以设置的，所以要存储初始值，频率发生改变时调用setRate方法即可，需要注意的是setRate的意义是每秒允许多少次访问，参数是一个double类型的值\n\n\t\tRateLimiter rateLimit = RestComponentFactory.interfaceRateMap.getOrDefault(interfaceVo.getToken(), null);\n\t\tDouble newRateLimit = interfaceVo.getQps();\n\t\tif(interfaceVo.getQps()!= null && interfaceVo.getQps() > 0){\n\t\t\tif(rateLimit == null){\n\t\t\t\trateLimit = RateLimiter.create(interfaceVo.getQps());\n\t\t\t\tRestComponentFactory.interfaceRateMap.put(interfaceVo.getToken(), rateLimit);\n\t\t\t}else{\n\t\t\t\tif(rateLimit.getRate() != newRateLimit){\n\t\t\t\t\trateLimit.setRate(newRateLimit);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tboolean rateFlag = rateLimit.tryAcquire();\n\t\t\tif(!rateFlag){\n\t\t\t\tthrow new Exception470();\n\t\t\t}\n\t\t}\n \n\n\n\n****补充说明**** \nRateLimiter类位于  \n\ncom.google.common.util.concurrent.RateLimiter\n\n\t<artifactId>guava</artifactId>\n\t<groupId>com.google.guava</groupId>\n\n\n\n\n\n​ \n​","source":"_posts/使用RateLimiter限制访问频率.md","raw":"---\ntitle: 使用RateLimiter限制访问频率   \ndate: 2019-06-12 17:23:44   \ncategories: \"Java\"  \ntags: [qps ratelimiter]    \ndescription: 使用RateLimiter限制访问频率\n\n---\n\n****背景****     \n通过查看接口访问日志发现某个注册接口请求量突然暴涨，并引发连锁反应，导致整个系统变慢。系统已经对接口做了做了权限认证，包含token认证，时效认证，禁用启用等功能，但没有对访问频率做限制，除了需要优化处理逻辑外，也要加上对接口的访问频率限制，以防止非预期的请求对系统压力过大而引起的系统瘫痪，当流量过大时，可以采取拒绝或者引流等机制。\n\n\n****原因及方案分析****    \n\n常用的限流算法有两种：漏桶算法和令牌桶算法。\n\n漏桶(Leaky Bucket)算法思路很简单,水(请求)先进入到漏桶里,漏桶以一定的速度出水(接口有响应速率),当水流入速度过大会直接溢出(访问频率超过接口响应速率),然后就拒绝请求,可以看出漏桶算法能强行限制数据的传输速率。但是漏桶的漏出速率是固定的参数,所以,即使网络中不存在资源冲突(没有发生拥塞),漏桶算法也不能使流突发(burst)到端口速率.因此,漏桶算法对于存在突发特性的流量来说缺乏效率.\n![](https://i.imgur.com/doxSzsv.png)    \n令牌桶算法(Token Bucket)和 Leaky Bucket 效果一样但方向相反的算法,更加容易理解.随着时间流逝,系统会按恒定1/QPS时间间隔(如果QPS=100,则间隔是10ms)往桶里加入Token(想象和漏洞漏水相反,有个水龙头在不断的加水),如果桶已经满了就不再加了.新请求来临时,会各自拿走一个Token,如果没有Token可拿了就阻塞或者拒绝服务.令牌桶的另外一个好处是可以方便的改变速度. 一旦需要提高速率,则按需提高放入桶中的令牌的速率. 一般会定时(比如100毫秒)往桶中增加一定数量的令牌, 有些变种算法则实时的计算应该增加的令牌的数量.\n\n![](https://i.imgur.com/WCQ32wH.png)\n****解决思路****\n\n使用令牌桶算法限制接口访问频率sss。\n\n****详细实现****    \n\t\n   Google开源工具包Guava提供了限流工具类RateLimiter，该类基于令牌桶算法来完成限流，非常易于使用。在项目中用了以下实现 \n\n   1.频率是可以设置的，所以要存储初始值，频率发生改变时调用setRate方法即可，需要注意的是setRate的意义是每秒允许多少次访问，参数是一个double类型的值\n\n\t\tRateLimiter rateLimit = RestComponentFactory.interfaceRateMap.getOrDefault(interfaceVo.getToken(), null);\n\t\tDouble newRateLimit = interfaceVo.getQps();\n\t\tif(interfaceVo.getQps()!= null && interfaceVo.getQps() > 0){\n\t\t\tif(rateLimit == null){\n\t\t\t\trateLimit = RateLimiter.create(interfaceVo.getQps());\n\t\t\t\tRestComponentFactory.interfaceRateMap.put(interfaceVo.getToken(), rateLimit);\n\t\t\t}else{\n\t\t\t\tif(rateLimit.getRate() != newRateLimit){\n\t\t\t\t\trateLimit.setRate(newRateLimit);\n\t\t\t\t}\n\t\t\t}\n\n\t\t\tboolean rateFlag = rateLimit.tryAcquire();\n\t\t\tif(!rateFlag){\n\t\t\t\tthrow new Exception470();\n\t\t\t}\n\t\t}\n \n\n\n\n****补充说明**** \nRateLimiter类位于  \n\ncom.google.common.util.concurrent.RateLimiter\n\n\t<artifactId>guava</artifactId>\n\t<groupId>com.google.guava</groupId>\n\n\n\n\n\n​ \n​","slug":"使用RateLimiter限制访问频率","published":1,"updated":"2019-09-29T08:04:56.769Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cli5o2dqw000aczzk5il8cl0s","content":"<p><strong><strong>背景</strong></strong><br>通过查看接口访问日志发现某个注册接口请求量突然暴涨，并引发连锁反应，导致整个系统变慢。系统已经对接口做了做了权限认证，包含token认证，时效认证，禁用启用等功能，但没有对访问频率做限制，除了需要优化处理逻辑外，也要加上对接口的访问频率限制，以防止非预期的请求对系统压力过大而引起的系统瘫痪，当流量过大时，可以采取拒绝或者引流等机制。</p>\n<p><strong><strong>原因及方案分析</strong></strong>    </p>\n<p>常用的限流算法有两种：漏桶算法和令牌桶算法。</p>\n<p>漏桶(Leaky Bucket)算法思路很简单,水(请求)先进入到漏桶里,漏桶以一定的速度出水(接口有响应速率),当水流入速度过大会直接溢出(访问频率超过接口响应速率),然后就拒绝请求,可以看出漏桶算法能强行限制数据的传输速率。但是漏桶的漏出速率是固定的参数,所以,即使网络中不存在资源冲突(没有发生拥塞),漏桶算法也不能使流突发(burst)到端口速率.因此,漏桶算法对于存在突发特性的流量来说缺乏效率.<br><img src=\"https://i.imgur.com/doxSzsv.png\"><br>令牌桶算法(Token Bucket)和 Leaky Bucket 效果一样但方向相反的算法,更加容易理解.随着时间流逝,系统会按恒定1&#x2F;QPS时间间隔(如果QPS&#x3D;100,则间隔是10ms)往桶里加入Token(想象和漏洞漏水相反,有个水龙头在不断的加水),如果桶已经满了就不再加了.新请求来临时,会各自拿走一个Token,如果没有Token可拿了就阻塞或者拒绝服务.令牌桶的另外一个好处是可以方便的改变速度. 一旦需要提高速率,则按需提高放入桶中的令牌的速率. 一般会定时(比如100毫秒)往桶中增加一定数量的令牌, 有些变种算法则实时的计算应该增加的令牌的数量.</p>\n<p><img src=\"https://i.imgur.com/WCQ32wH.png\"><br><strong><strong>解决思路</strong></strong></p>\n<p>使用令牌桶算法限制接口访问频率sss。</p>\n<p><strong><strong>详细实现</strong></strong>    </p>\n<p>   Google开源工具包Guava提供了限流工具类RateLimiter，该类基于令牌桶算法来完成限流，非常易于使用。在项目中用了以下实现 </p>\n<p>   1.频率是可以设置的，所以要存储初始值，频率发生改变时调用setRate方法即可，需要注意的是setRate的意义是每秒允许多少次访问，参数是一个double类型的值</p>\n<pre><code>    RateLimiter rateLimit = RestComponentFactory.interfaceRateMap.getOrDefault(interfaceVo.getToken(), null);\n    Double newRateLimit = interfaceVo.getQps();\n    if(interfaceVo.getQps()!= null &amp;&amp; interfaceVo.getQps() &gt; 0)&#123;\n        if(rateLimit == null)&#123;\n            rateLimit = RateLimiter.create(interfaceVo.getQps());\n            RestComponentFactory.interfaceRateMap.put(interfaceVo.getToken(), rateLimit);\n        &#125;else&#123;\n            if(rateLimit.getRate() != newRateLimit)&#123;\n                rateLimit.setRate(newRateLimit);\n            &#125;\n        &#125;\n\n        boolean rateFlag = rateLimit.tryAcquire();\n        if(!rateFlag)&#123;\n            throw new Exception470();\n        &#125;\n    &#125;\n</code></pre>\n<p><strong><strong>补充说明</strong></strong><br>RateLimiter类位于  </p>\n<p>com.google.common.util.concurrent.RateLimiter</p>\n<pre><code>&lt;artifactId&gt;guava&lt;/artifactId&gt;\n&lt;groupId&gt;com.google.guava&lt;/groupId&gt;\n</code></pre>\n<p>​<br>​</p>\n","site":{"data":{}},"cover":false,"excerpt":"","more":"<p><strong><strong>背景</strong></strong><br>通过查看接口访问日志发现某个注册接口请求量突然暴涨，并引发连锁反应，导致整个系统变慢。系统已经对接口做了做了权限认证，包含token认证，时效认证，禁用启用等功能，但没有对访问频率做限制，除了需要优化处理逻辑外，也要加上对接口的访问频率限制，以防止非预期的请求对系统压力过大而引起的系统瘫痪，当流量过大时，可以采取拒绝或者引流等机制。</p>\n<p><strong><strong>原因及方案分析</strong></strong>    </p>\n<p>常用的限流算法有两种：漏桶算法和令牌桶算法。</p>\n<p>漏桶(Leaky Bucket)算法思路很简单,水(请求)先进入到漏桶里,漏桶以一定的速度出水(接口有响应速率),当水流入速度过大会直接溢出(访问频率超过接口响应速率),然后就拒绝请求,可以看出漏桶算法能强行限制数据的传输速率。但是漏桶的漏出速率是固定的参数,所以,即使网络中不存在资源冲突(没有发生拥塞),漏桶算法也不能使流突发(burst)到端口速率.因此,漏桶算法对于存在突发特性的流量来说缺乏效率.<br><img src=\"https://i.imgur.com/doxSzsv.png\"><br>令牌桶算法(Token Bucket)和 Leaky Bucket 效果一样但方向相反的算法,更加容易理解.随着时间流逝,系统会按恒定1&#x2F;QPS时间间隔(如果QPS&#x3D;100,则间隔是10ms)往桶里加入Token(想象和漏洞漏水相反,有个水龙头在不断的加水),如果桶已经满了就不再加了.新请求来临时,会各自拿走一个Token,如果没有Token可拿了就阻塞或者拒绝服务.令牌桶的另外一个好处是可以方便的改变速度. 一旦需要提高速率,则按需提高放入桶中的令牌的速率. 一般会定时(比如100毫秒)往桶中增加一定数量的令牌, 有些变种算法则实时的计算应该增加的令牌的数量.</p>\n<p><img src=\"https://i.imgur.com/WCQ32wH.png\"><br><strong><strong>解决思路</strong></strong></p>\n<p>使用令牌桶算法限制接口访问频率sss。</p>\n<p><strong><strong>详细实现</strong></strong>    </p>\n<p>   Google开源工具包Guava提供了限流工具类RateLimiter，该类基于令牌桶算法来完成限流，非常易于使用。在项目中用了以下实现 </p>\n<p>   1.频率是可以设置的，所以要存储初始值，频率发生改变时调用setRate方法即可，需要注意的是setRate的意义是每秒允许多少次访问，参数是一个double类型的值</p>\n<pre><code>    RateLimiter rateLimit = RestComponentFactory.interfaceRateMap.getOrDefault(interfaceVo.getToken(), null);\n    Double newRateLimit = interfaceVo.getQps();\n    if(interfaceVo.getQps()!= null &amp;&amp; interfaceVo.getQps() &gt; 0)&#123;\n        if(rateLimit == null)&#123;\n            rateLimit = RateLimiter.create(interfaceVo.getQps());\n            RestComponentFactory.interfaceRateMap.put(interfaceVo.getToken(), rateLimit);\n        &#125;else&#123;\n            if(rateLimit.getRate() != newRateLimit)&#123;\n                rateLimit.setRate(newRateLimit);\n            &#125;\n        &#125;\n\n        boolean rateFlag = rateLimit.tryAcquire();\n        if(!rateFlag)&#123;\n            throw new Exception470();\n        &#125;\n    &#125;\n</code></pre>\n<p><strong><strong>补充说明</strong></strong><br>RateLimiter类位于  </p>\n<p>com.google.common.util.concurrent.RateLimiter</p>\n<pre><code>&lt;artifactId&gt;guava&lt;/artifactId&gt;\n&lt;groupId&gt;com.google.guava&lt;/groupId&gt;\n</code></pre>\n<p>​<br>​</p>\n"},{"title":"为什么中国的 996 干不过美国的 955","date":"2020-05-29T11:50:32.000Z","description":"为什么中国的 996 干不过美国的 955","_content":">看到了老板发的文章，特别有感触，中国的SaaS个性化需求太多，付费意愿低下也是影响 2B 企业战略的很大因素。导致很多企业服务厂商要给客户做大量定制。这个问题的原因是中国企业管理不规范，老板文化太重。\n\n> 原文:  \n\n为什么中国的 996 干不过美国的 955\n\n大概从 2019 年上半年开始，有一个问题一直困扰着我，在 2B 领域，为啥中国的公司如此努力的加班，但是在产品上还是不能跟国外的比？虽然我们公司不是加班很多，但是相比国外同类公司还是工作时长要长很多，我们最近为了赶持续部署产品上线，相关团队基本上是 997。腾讯云，阿里云，我了解到的情况也是加班比较多的，996 的情况不在少数。但是你看中美两国云服务的产品，显然国外的还是要比国内好一大截。把这个问题放大一点，在中美两国贸易战的背景下，“为什么中国的 996 干不过美国的 955？”。我们从三个方面来看这个问题。\n![](/img/page/996/img.png)  \n一、中国 IT 从业者的素质到底行不行？\n\n对比中国的一线城市，我觉得从业者的专业水平并不差。我十年前在硅谷工作，大量的印度大妈的 Title 都是 Principal Engineer，这大概也是我当初决定回国的原因之一。近几年，有大量工程师在中国工作一段时间后去美国的，以及在美国工作一段时间后回国的。我感觉在从业者素质方面，高端人才两边是差不多的。但是中国实在是人多，论数量，初级从业者肯定更多，所以在日常感觉上，国内的整体素质没有国外高，我认为在我们讨论的业务范围，这是一种错觉。\n\n但是国内从业者在专业度确实是欠缺的。这点从简历就能看出来。中国即使有些很高级的工程师，看简历也是写的不合格的，从内容到格式都不合格。但是你去国外招聘，无论水平多烂，简历都是写的非常漂亮。国外在如何写简历这件事情上的培训是相当成熟的，值得我们学习。\n\n另外在理解自己与公司的关系上，国内普遍带有更多情感，而国外更理性。这一点要展开就太大了，要从文化开始讲。但是无论如何，我认为这个差异只影响管理方式，不应该在产出质量上有大的影响。\n\n二、中国的 IT 从业者是不是不够努力？\n\n这个问题是显而易见的，但我还是想说一下。每次晚上 10 点左右，在公司楼下打车，滴滴告诉你排队 101 号，一线城市都这样，深圳科技园只是一个缩影。我们说 996，只是一个代名词，有可能是 10-10-6，但起码我从来没有见过民营 IT 企业是 955 的。除此以外，国内大量公司在非工作时间是要处理公事的，也就是公私没有那么分明。以至于企业微信每周都会给你一个报告，告诉你这一周最晚处理公事对话是几点。在朋友圈晒凌晨两三点的不在少数吧？\n\n即使是这样，相比其他行业，IT 行业可能都是工作时间最短的。建筑行业，工厂制造业，基本上一年就休一个春节。中国这么多人，每个人每天都有 24 小时，中国这些年的 GDP 增长，就是这么靠堆时间堆出来的。但是堆时间的杠杆率越来越低了，所以 GDP 的增速下降很快。\n\n三、中国 2B 公司的战略行不行？\n\n产品做不好，如果不是人的问题，不是投入度问题，那大概只剩下方向问题了。我们先来看一组截图：\n\n![](/img/page/996/img_1.png)\n\n对比国内外几个云厂商的首页，其实问题还挺明显的。首页就是门面，代表了你的形象，代表了你想传递什么信息给客户。美国的厂商基本上是在讲产品和技术，而中国云厂商基本上是在搞促销，而且“吃相很难看”。我一直认为作为一个面向企业的专业服务商，参加双十一这样的活动就是一种耻辱。我从来没见那个美国的企业服务公司参加黑五。你看阿里云的首页在视觉观感上是不是越来越像淘宝？相由心生啊！这么搞如何塑造专业形象呢？没有专业形象，附加值就上不去，所以生意越做越苦逼。“IBM，Oracle，微软，Google，AWS，阿里云，腾讯云，华为云”这几个 LOGO 大家看到时候我相信内心的感觉是不一样的。我觉得这就是差距。\n\n![](/img/page/996/img_3.png)\n\n阿里这种强运营的做法，确实能获得更多客户，其他厂商也不得不跟进，就导致了现在局面：中国特色运营驱动的云计算产业。业务如果靠销售驱动，杠杆率最低，运营其次，产品最高。所以你去看销售驱动的公司，往往有很多人，但是人均产值很低，比如地产，软件外包。但是产品驱动的公司/业务，往往有超高人均产值，比如微信。再比如国外的 Atlassian（典型产品 JIRA），目前市值 276 亿美元，员工 3600 人，人均市值 920 万美元。这家公司没有销售。\n\n我们在做绩效考核的时候有一句话：你考核什么你就会得到什么。再放大一点，“这个社会奖励什么，就会得到什么”。我想来想去，觉得这里的核心不是供给的问题，而是需求的问题。国内目前的 IT 大环境实质上是鼓励 60 分产品，不重视 80 分或者 100 分产品。花一份精力，可以把产品做到 60 分；花两份精力可以把产品做到 80 分；但是花 10 份精力才能做到 100 分。如果没有足够的正向激励，没有人会去做 100 分产品。所以中国市场上有很多 60 分产品，鲜有 80 分产品，没有 100 分产品。做过企业级项目打单，招投标的人想来都明白，能不能拿下客户，产品最多只有一半的因素。而且很多企业目前追求的是能用，并没有那么强烈的品质诉求。\n\n这个问题说到底也不是 IT 行业的问题。我觉得更明显的是在建筑行业。中国 30 年时间盖了比别人 200 年还要多的房子，可以想象这个房屋质量。每次去老牌资本主义国家，看到人家的写字楼，虽然已经上百年，但那种品质感，国内少有。全球云计算领头羊 AWS 成立于 2006 年，刚才说的 Atlassian 成立于 2002 年。产品驱动就是前面的投入很大，时间很长。我们追赶追求短平快，品质也难以做好。\n![](/img/page/996/img_2.png)\n\n除此以外，个性化需求太多，付费意愿低下也是影响 2B 企业战略的很大因素。中国企业的个性化诉求太多了，导致很多企业服务厂商要给客户做大量定制。这个问题的原因是中国企业管理不规范，老板文化太重。但我相信这个问题正在改善，包括企业微信/钉钉在内的各种工具在倒逼中小企业的管理规范化，标准化。付费意愿也是，可以看到改善，但是依然很低。CODING 十个活跃用户里面，只有一个是付费的。如果产品你用了，仍然不付费，那企业只能想别的办法搞钱。\n\n以上种种，导致了想要在国内这个环境慢慢打磨产品变得不可能，基本上会死在路上。所以只能逼良为娼，先污染，后治理。虽然中国企业可能投入的人更多，但是抛开工程师的素质不谈，同样多的人力做一个产品和同时做两个产品，差距还是很大的。中国的 996 干不过美国的 955 还是路径选择问题，两条路的杠杆率差距太大。\n\n包括巨头云厂商在内的各个企业，我相信从老板开始，都很清楚做好产品是王道，但实践就是非常困难，把各大云厂商的收入构成扒开看一看就知道究竟有多少真正是产品带来的收入。既要速度，又要质量，还舍不得短期利益，真的可以兼得吗？最近在消费品领域出现了各种“升级”，国产高品质商品迅速崛起，这是新一代中产阶级的需求拉动的。我相信软件行业会出现类似的升级，“Made in China”形象的提升一定不仅仅在制造业。\n\n![](/img/page/996/img_4.png)\n\n\n\n","source":"_posts/为什么中国的996干不过美国的955.md","raw":"---\ntitle: 为什么中国的 996 干不过美国的 955   \ndate: 2020-05-29 19:50:32 \ncategories: \"SaaS\"  \ntags: [saas 996]    \ndescription: 为什么中国的 996 干不过美国的 955 \n\n---\n>看到了老板发的文章，特别有感触，中国的SaaS个性化需求太多，付费意愿低下也是影响 2B 企业战略的很大因素。导致很多企业服务厂商要给客户做大量定制。这个问题的原因是中国企业管理不规范，老板文化太重。\n\n> 原文:  \n\n为什么中国的 996 干不过美国的 955\n\n大概从 2019 年上半年开始，有一个问题一直困扰着我，在 2B 领域，为啥中国的公司如此努力的加班，但是在产品上还是不能跟国外的比？虽然我们公司不是加班很多，但是相比国外同类公司还是工作时长要长很多，我们最近为了赶持续部署产品上线，相关团队基本上是 997。腾讯云，阿里云，我了解到的情况也是加班比较多的，996 的情况不在少数。但是你看中美两国云服务的产品，显然国外的还是要比国内好一大截。把这个问题放大一点，在中美两国贸易战的背景下，“为什么中国的 996 干不过美国的 955？”。我们从三个方面来看这个问题。\n![](/img/page/996/img.png)  \n一、中国 IT 从业者的素质到底行不行？\n\n对比中国的一线城市，我觉得从业者的专业水平并不差。我十年前在硅谷工作，大量的印度大妈的 Title 都是 Principal Engineer，这大概也是我当初决定回国的原因之一。近几年，有大量工程师在中国工作一段时间后去美国的，以及在美国工作一段时间后回国的。我感觉在从业者素质方面，高端人才两边是差不多的。但是中国实在是人多，论数量，初级从业者肯定更多，所以在日常感觉上，国内的整体素质没有国外高，我认为在我们讨论的业务范围，这是一种错觉。\n\n但是国内从业者在专业度确实是欠缺的。这点从简历就能看出来。中国即使有些很高级的工程师，看简历也是写的不合格的，从内容到格式都不合格。但是你去国外招聘，无论水平多烂，简历都是写的非常漂亮。国外在如何写简历这件事情上的培训是相当成熟的，值得我们学习。\n\n另外在理解自己与公司的关系上，国内普遍带有更多情感，而国外更理性。这一点要展开就太大了，要从文化开始讲。但是无论如何，我认为这个差异只影响管理方式，不应该在产出质量上有大的影响。\n\n二、中国的 IT 从业者是不是不够努力？\n\n这个问题是显而易见的，但我还是想说一下。每次晚上 10 点左右，在公司楼下打车，滴滴告诉你排队 101 号，一线城市都这样，深圳科技园只是一个缩影。我们说 996，只是一个代名词，有可能是 10-10-6，但起码我从来没有见过民营 IT 企业是 955 的。除此以外，国内大量公司在非工作时间是要处理公事的，也就是公私没有那么分明。以至于企业微信每周都会给你一个报告，告诉你这一周最晚处理公事对话是几点。在朋友圈晒凌晨两三点的不在少数吧？\n\n即使是这样，相比其他行业，IT 行业可能都是工作时间最短的。建筑行业，工厂制造业，基本上一年就休一个春节。中国这么多人，每个人每天都有 24 小时，中国这些年的 GDP 增长，就是这么靠堆时间堆出来的。但是堆时间的杠杆率越来越低了，所以 GDP 的增速下降很快。\n\n三、中国 2B 公司的战略行不行？\n\n产品做不好，如果不是人的问题，不是投入度问题，那大概只剩下方向问题了。我们先来看一组截图：\n\n![](/img/page/996/img_1.png)\n\n对比国内外几个云厂商的首页，其实问题还挺明显的。首页就是门面，代表了你的形象，代表了你想传递什么信息给客户。美国的厂商基本上是在讲产品和技术，而中国云厂商基本上是在搞促销，而且“吃相很难看”。我一直认为作为一个面向企业的专业服务商，参加双十一这样的活动就是一种耻辱。我从来没见那个美国的企业服务公司参加黑五。你看阿里云的首页在视觉观感上是不是越来越像淘宝？相由心生啊！这么搞如何塑造专业形象呢？没有专业形象，附加值就上不去，所以生意越做越苦逼。“IBM，Oracle，微软，Google，AWS，阿里云，腾讯云，华为云”这几个 LOGO 大家看到时候我相信内心的感觉是不一样的。我觉得这就是差距。\n\n![](/img/page/996/img_3.png)\n\n阿里这种强运营的做法，确实能获得更多客户，其他厂商也不得不跟进，就导致了现在局面：中国特色运营驱动的云计算产业。业务如果靠销售驱动，杠杆率最低，运营其次，产品最高。所以你去看销售驱动的公司，往往有很多人，但是人均产值很低，比如地产，软件外包。但是产品驱动的公司/业务，往往有超高人均产值，比如微信。再比如国外的 Atlassian（典型产品 JIRA），目前市值 276 亿美元，员工 3600 人，人均市值 920 万美元。这家公司没有销售。\n\n我们在做绩效考核的时候有一句话：你考核什么你就会得到什么。再放大一点，“这个社会奖励什么，就会得到什么”。我想来想去，觉得这里的核心不是供给的问题，而是需求的问题。国内目前的 IT 大环境实质上是鼓励 60 分产品，不重视 80 分或者 100 分产品。花一份精力，可以把产品做到 60 分；花两份精力可以把产品做到 80 分；但是花 10 份精力才能做到 100 分。如果没有足够的正向激励，没有人会去做 100 分产品。所以中国市场上有很多 60 分产品，鲜有 80 分产品，没有 100 分产品。做过企业级项目打单，招投标的人想来都明白，能不能拿下客户，产品最多只有一半的因素。而且很多企业目前追求的是能用，并没有那么强烈的品质诉求。\n\n这个问题说到底也不是 IT 行业的问题。我觉得更明显的是在建筑行业。中国 30 年时间盖了比别人 200 年还要多的房子，可以想象这个房屋质量。每次去老牌资本主义国家，看到人家的写字楼，虽然已经上百年，但那种品质感，国内少有。全球云计算领头羊 AWS 成立于 2006 年，刚才说的 Atlassian 成立于 2002 年。产品驱动就是前面的投入很大，时间很长。我们追赶追求短平快，品质也难以做好。\n![](/img/page/996/img_2.png)\n\n除此以外，个性化需求太多，付费意愿低下也是影响 2B 企业战略的很大因素。中国企业的个性化诉求太多了，导致很多企业服务厂商要给客户做大量定制。这个问题的原因是中国企业管理不规范，老板文化太重。但我相信这个问题正在改善，包括企业微信/钉钉在内的各种工具在倒逼中小企业的管理规范化，标准化。付费意愿也是，可以看到改善，但是依然很低。CODING 十个活跃用户里面，只有一个是付费的。如果产品你用了，仍然不付费，那企业只能想别的办法搞钱。\n\n以上种种，导致了想要在国内这个环境慢慢打磨产品变得不可能，基本上会死在路上。所以只能逼良为娼，先污染，后治理。虽然中国企业可能投入的人更多，但是抛开工程师的素质不谈，同样多的人力做一个产品和同时做两个产品，差距还是很大的。中国的 996 干不过美国的 955 还是路径选择问题，两条路的杠杆率差距太大。\n\n包括巨头云厂商在内的各个企业，我相信从老板开始，都很清楚做好产品是王道，但实践就是非常困难，把各大云厂商的收入构成扒开看一看就知道究竟有多少真正是产品带来的收入。既要速度，又要质量，还舍不得短期利益，真的可以兼得吗？最近在消费品领域出现了各种“升级”，国产高品质商品迅速崛起，这是新一代中产阶级的需求拉动的。我相信软件行业会出现类似的升级，“Made in China”形象的提升一定不仅仅在制造业。\n\n![](/img/page/996/img_4.png)\n\n\n\n","slug":"为什么中国的996干不过美国的955","published":1,"updated":"2023-05-27T04:12:39.266Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cli5o2dqx000bczzk3x63g4lt","content":"<blockquote>\n<p>看到了老板发的文章，特别有感触，中国的SaaS个性化需求太多，付费意愿低下也是影响 2B 企业战略的很大因素。导致很多企业服务厂商要给客户做大量定制。这个问题的原因是中国企业管理不规范，老板文化太重。</p>\n</blockquote>\n<blockquote>\n<p>原文:  </p>\n</blockquote>\n<p>为什么中国的 996 干不过美国的 955</p>\n<p>大概从 2019 年上半年开始，有一个问题一直困扰着我，在 2B 领域，为啥中国的公司如此努力的加班，但是在产品上还是不能跟国外的比？虽然我们公司不是加班很多，但是相比国外同类公司还是工作时长要长很多，我们最近为了赶持续部署产品上线，相关团队基本上是 997。腾讯云，阿里云，我了解到的情况也是加班比较多的，996 的情况不在少数。但是你看中美两国云服务的产品，显然国外的还是要比国内好一大截。把这个问题放大一点，在中美两国贸易战的背景下，“为什么中国的 996 干不过美国的 955？”。我们从三个方面来看这个问题。<br><img src=\"/img/page/996/img.png\"><br>一、中国 IT 从业者的素质到底行不行？</p>\n<p>对比中国的一线城市，我觉得从业者的专业水平并不差。我十年前在硅谷工作，大量的印度大妈的 Title 都是 Principal Engineer，这大概也是我当初决定回国的原因之一。近几年，有大量工程师在中国工作一段时间后去美国的，以及在美国工作一段时间后回国的。我感觉在从业者素质方面，高端人才两边是差不多的。但是中国实在是人多，论数量，初级从业者肯定更多，所以在日常感觉上，国内的整体素质没有国外高，我认为在我们讨论的业务范围，这是一种错觉。</p>\n<p>但是国内从业者在专业度确实是欠缺的。这点从简历就能看出来。中国即使有些很高级的工程师，看简历也是写的不合格的，从内容到格式都不合格。但是你去国外招聘，无论水平多烂，简历都是写的非常漂亮。国外在如何写简历这件事情上的培训是相当成熟的，值得我们学习。</p>\n<p>另外在理解自己与公司的关系上，国内普遍带有更多情感，而国外更理性。这一点要展开就太大了，要从文化开始讲。但是无论如何，我认为这个差异只影响管理方式，不应该在产出质量上有大的影响。</p>\n<p>二、中国的 IT 从业者是不是不够努力？</p>\n<p>这个问题是显而易见的，但我还是想说一下。每次晚上 10 点左右，在公司楼下打车，滴滴告诉你排队 101 号，一线城市都这样，深圳科技园只是一个缩影。我们说 996，只是一个代名词，有可能是 10-10-6，但起码我从来没有见过民营 IT 企业是 955 的。除此以外，国内大量公司在非工作时间是要处理公事的，也就是公私没有那么分明。以至于企业微信每周都会给你一个报告，告诉你这一周最晚处理公事对话是几点。在朋友圈晒凌晨两三点的不在少数吧？</p>\n<p>即使是这样，相比其他行业，IT 行业可能都是工作时间最短的。建筑行业，工厂制造业，基本上一年就休一个春节。中国这么多人，每个人每天都有 24 小时，中国这些年的 GDP 增长，就是这么靠堆时间堆出来的。但是堆时间的杠杆率越来越低了，所以 GDP 的增速下降很快。</p>\n<p>三、中国 2B 公司的战略行不行？</p>\n<p>产品做不好，如果不是人的问题，不是投入度问题，那大概只剩下方向问题了。我们先来看一组截图：</p>\n<p><img src=\"/img/page/996/img_1.png\"></p>\n<p>对比国内外几个云厂商的首页，其实问题还挺明显的。首页就是门面，代表了你的形象，代表了你想传递什么信息给客户。美国的厂商基本上是在讲产品和技术，而中国云厂商基本上是在搞促销，而且“吃相很难看”。我一直认为作为一个面向企业的专业服务商，参加双十一这样的活动就是一种耻辱。我从来没见那个美国的企业服务公司参加黑五。你看阿里云的首页在视觉观感上是不是越来越像淘宝？相由心生啊！这么搞如何塑造专业形象呢？没有专业形象，附加值就上不去，所以生意越做越苦逼。“IBM，Oracle，微软，Google，AWS，阿里云，腾讯云，华为云”这几个 LOGO 大家看到时候我相信内心的感觉是不一样的。我觉得这就是差距。</p>\n<p><img src=\"/img/page/996/img_3.png\"></p>\n<p>阿里这种强运营的做法，确实能获得更多客户，其他厂商也不得不跟进，就导致了现在局面：中国特色运营驱动的云计算产业。业务如果靠销售驱动，杠杆率最低，运营其次，产品最高。所以你去看销售驱动的公司，往往有很多人，但是人均产值很低，比如地产，软件外包。但是产品驱动的公司&#x2F;业务，往往有超高人均产值，比如微信。再比如国外的 Atlassian（典型产品 JIRA），目前市值 276 亿美元，员工 3600 人，人均市值 920 万美元。这家公司没有销售。</p>\n<p>我们在做绩效考核的时候有一句话：你考核什么你就会得到什么。再放大一点，“这个社会奖励什么，就会得到什么”。我想来想去，觉得这里的核心不是供给的问题，而是需求的问题。国内目前的 IT 大环境实质上是鼓励 60 分产品，不重视 80 分或者 100 分产品。花一份精力，可以把产品做到 60 分；花两份精力可以把产品做到 80 分；但是花 10 份精力才能做到 100 分。如果没有足够的正向激励，没有人会去做 100 分产品。所以中国市场上有很多 60 分产品，鲜有 80 分产品，没有 100 分产品。做过企业级项目打单，招投标的人想来都明白，能不能拿下客户，产品最多只有一半的因素。而且很多企业目前追求的是能用，并没有那么强烈的品质诉求。</p>\n<p>这个问题说到底也不是 IT 行业的问题。我觉得更明显的是在建筑行业。中国 30 年时间盖了比别人 200 年还要多的房子，可以想象这个房屋质量。每次去老牌资本主义国家，看到人家的写字楼，虽然已经上百年，但那种品质感，国内少有。全球云计算领头羊 AWS 成立于 2006 年，刚才说的 Atlassian 成立于 2002 年。产品驱动就是前面的投入很大，时间很长。我们追赶追求短平快，品质也难以做好。<br><img src=\"/img/page/996/img_2.png\"></p>\n<p>除此以外，个性化需求太多，付费意愿低下也是影响 2B 企业战略的很大因素。中国企业的个性化诉求太多了，导致很多企业服务厂商要给客户做大量定制。这个问题的原因是中国企业管理不规范，老板文化太重。但我相信这个问题正在改善，包括企业微信&#x2F;钉钉在内的各种工具在倒逼中小企业的管理规范化，标准化。付费意愿也是，可以看到改善，但是依然很低。CODING 十个活跃用户里面，只有一个是付费的。如果产品你用了，仍然不付费，那企业只能想别的办法搞钱。</p>\n<p>以上种种，导致了想要在国内这个环境慢慢打磨产品变得不可能，基本上会死在路上。所以只能逼良为娼，先污染，后治理。虽然中国企业可能投入的人更多，但是抛开工程师的素质不谈，同样多的人力做一个产品和同时做两个产品，差距还是很大的。中国的 996 干不过美国的 955 还是路径选择问题，两条路的杠杆率差距太大。</p>\n<p>包括巨头云厂商在内的各个企业，我相信从老板开始，都很清楚做好产品是王道，但实践就是非常困难，把各大云厂商的收入构成扒开看一看就知道究竟有多少真正是产品带来的收入。既要速度，又要质量，还舍不得短期利益，真的可以兼得吗？最近在消费品领域出现了各种“升级”，国产高品质商品迅速崛起，这是新一代中产阶级的需求拉动的。我相信软件行业会出现类似的升级，“Made in China”形象的提升一定不仅仅在制造业。</p>\n<p><img src=\"/img/page/996/img_4.png\"></p>\n","site":{"data":{}},"cover":false,"excerpt":"","more":"<blockquote>\n<p>看到了老板发的文章，特别有感触，中国的SaaS个性化需求太多，付费意愿低下也是影响 2B 企业战略的很大因素。导致很多企业服务厂商要给客户做大量定制。这个问题的原因是中国企业管理不规范，老板文化太重。</p>\n</blockquote>\n<blockquote>\n<p>原文:  </p>\n</blockquote>\n<p>为什么中国的 996 干不过美国的 955</p>\n<p>大概从 2019 年上半年开始，有一个问题一直困扰着我，在 2B 领域，为啥中国的公司如此努力的加班，但是在产品上还是不能跟国外的比？虽然我们公司不是加班很多，但是相比国外同类公司还是工作时长要长很多，我们最近为了赶持续部署产品上线，相关团队基本上是 997。腾讯云，阿里云，我了解到的情况也是加班比较多的，996 的情况不在少数。但是你看中美两国云服务的产品，显然国外的还是要比国内好一大截。把这个问题放大一点，在中美两国贸易战的背景下，“为什么中国的 996 干不过美国的 955？”。我们从三个方面来看这个问题。<br><img src=\"/img/page/996/img.png\"><br>一、中国 IT 从业者的素质到底行不行？</p>\n<p>对比中国的一线城市，我觉得从业者的专业水平并不差。我十年前在硅谷工作，大量的印度大妈的 Title 都是 Principal Engineer，这大概也是我当初决定回国的原因之一。近几年，有大量工程师在中国工作一段时间后去美国的，以及在美国工作一段时间后回国的。我感觉在从业者素质方面，高端人才两边是差不多的。但是中国实在是人多，论数量，初级从业者肯定更多，所以在日常感觉上，国内的整体素质没有国外高，我认为在我们讨论的业务范围，这是一种错觉。</p>\n<p>但是国内从业者在专业度确实是欠缺的。这点从简历就能看出来。中国即使有些很高级的工程师，看简历也是写的不合格的，从内容到格式都不合格。但是你去国外招聘，无论水平多烂，简历都是写的非常漂亮。国外在如何写简历这件事情上的培训是相当成熟的，值得我们学习。</p>\n<p>另外在理解自己与公司的关系上，国内普遍带有更多情感，而国外更理性。这一点要展开就太大了，要从文化开始讲。但是无论如何，我认为这个差异只影响管理方式，不应该在产出质量上有大的影响。</p>\n<p>二、中国的 IT 从业者是不是不够努力？</p>\n<p>这个问题是显而易见的，但我还是想说一下。每次晚上 10 点左右，在公司楼下打车，滴滴告诉你排队 101 号，一线城市都这样，深圳科技园只是一个缩影。我们说 996，只是一个代名词，有可能是 10-10-6，但起码我从来没有见过民营 IT 企业是 955 的。除此以外，国内大量公司在非工作时间是要处理公事的，也就是公私没有那么分明。以至于企业微信每周都会给你一个报告，告诉你这一周最晚处理公事对话是几点。在朋友圈晒凌晨两三点的不在少数吧？</p>\n<p>即使是这样，相比其他行业，IT 行业可能都是工作时间最短的。建筑行业，工厂制造业，基本上一年就休一个春节。中国这么多人，每个人每天都有 24 小时，中国这些年的 GDP 增长，就是这么靠堆时间堆出来的。但是堆时间的杠杆率越来越低了，所以 GDP 的增速下降很快。</p>\n<p>三、中国 2B 公司的战略行不行？</p>\n<p>产品做不好，如果不是人的问题，不是投入度问题，那大概只剩下方向问题了。我们先来看一组截图：</p>\n<p><img src=\"/img/page/996/img_1.png\"></p>\n<p>对比国内外几个云厂商的首页，其实问题还挺明显的。首页就是门面，代表了你的形象，代表了你想传递什么信息给客户。美国的厂商基本上是在讲产品和技术，而中国云厂商基本上是在搞促销，而且“吃相很难看”。我一直认为作为一个面向企业的专业服务商，参加双十一这样的活动就是一种耻辱。我从来没见那个美国的企业服务公司参加黑五。你看阿里云的首页在视觉观感上是不是越来越像淘宝？相由心生啊！这么搞如何塑造专业形象呢？没有专业形象，附加值就上不去，所以生意越做越苦逼。“IBM，Oracle，微软，Google，AWS，阿里云，腾讯云，华为云”这几个 LOGO 大家看到时候我相信内心的感觉是不一样的。我觉得这就是差距。</p>\n<p><img src=\"/img/page/996/img_3.png\"></p>\n<p>阿里这种强运营的做法，确实能获得更多客户，其他厂商也不得不跟进，就导致了现在局面：中国特色运营驱动的云计算产业。业务如果靠销售驱动，杠杆率最低，运营其次，产品最高。所以你去看销售驱动的公司，往往有很多人，但是人均产值很低，比如地产，软件外包。但是产品驱动的公司&#x2F;业务，往往有超高人均产值，比如微信。再比如国外的 Atlassian（典型产品 JIRA），目前市值 276 亿美元，员工 3600 人，人均市值 920 万美元。这家公司没有销售。</p>\n<p>我们在做绩效考核的时候有一句话：你考核什么你就会得到什么。再放大一点，“这个社会奖励什么，就会得到什么”。我想来想去，觉得这里的核心不是供给的问题，而是需求的问题。国内目前的 IT 大环境实质上是鼓励 60 分产品，不重视 80 分或者 100 分产品。花一份精力，可以把产品做到 60 分；花两份精力可以把产品做到 80 分；但是花 10 份精力才能做到 100 分。如果没有足够的正向激励，没有人会去做 100 分产品。所以中国市场上有很多 60 分产品，鲜有 80 分产品，没有 100 分产品。做过企业级项目打单，招投标的人想来都明白，能不能拿下客户，产品最多只有一半的因素。而且很多企业目前追求的是能用，并没有那么强烈的品质诉求。</p>\n<p>这个问题说到底也不是 IT 行业的问题。我觉得更明显的是在建筑行业。中国 30 年时间盖了比别人 200 年还要多的房子，可以想象这个房屋质量。每次去老牌资本主义国家，看到人家的写字楼，虽然已经上百年，但那种品质感，国内少有。全球云计算领头羊 AWS 成立于 2006 年，刚才说的 Atlassian 成立于 2002 年。产品驱动就是前面的投入很大，时间很长。我们追赶追求短平快，品质也难以做好。<br><img src=\"/img/page/996/img_2.png\"></p>\n<p>除此以外，个性化需求太多，付费意愿低下也是影响 2B 企业战略的很大因素。中国企业的个性化诉求太多了，导致很多企业服务厂商要给客户做大量定制。这个问题的原因是中国企业管理不规范，老板文化太重。但我相信这个问题正在改善，包括企业微信&#x2F;钉钉在内的各种工具在倒逼中小企业的管理规范化，标准化。付费意愿也是，可以看到改善，但是依然很低。CODING 十个活跃用户里面，只有一个是付费的。如果产品你用了，仍然不付费，那企业只能想别的办法搞钱。</p>\n<p>以上种种，导致了想要在国内这个环境慢慢打磨产品变得不可能，基本上会死在路上。所以只能逼良为娼，先污染，后治理。虽然中国企业可能投入的人更多，但是抛开工程师的素质不谈，同样多的人力做一个产品和同时做两个产品，差距还是很大的。中国的 996 干不过美国的 955 还是路径选择问题，两条路的杠杆率差距太大。</p>\n<p>包括巨头云厂商在内的各个企业，我相信从老板开始，都很清楚做好产品是王道，但实践就是非常困难，把各大云厂商的收入构成扒开看一看就知道究竟有多少真正是产品带来的收入。既要速度，又要质量，还舍不得短期利益，真的可以兼得吗？最近在消费品领域出现了各种“升级”，国产高品质商品迅速崛起，这是新一代中产阶级的需求拉动的。我相信软件行业会出现类似的升级，“Made in China”形象的提升一定不仅仅在制造业。</p>\n<p><img src=\"/img/page/996/img_4.png\"></p>\n"}],"PostAsset":[],"PostCategory":[{"post_id":"cli5o2dqf0000czzkdml80vjp","category_id":"cli5o2dqo0002czzk03wyfpi5","_id":"cli5o2dqz000dczzkafie6uim"},{"post_id":"cli5o2dqw000aczzk5il8cl0s","category_id":"cli5o2dqo0002czzk03wyfpi5","_id":"cli5o2dr0000gczzkgpg8d89b"},{"post_id":"cli5o2dql0001czzkckq1fhn1","category_id":"cli5o2dqo0002czzk03wyfpi5","_id":"cli5o2dr1000jczzkg6vm3hc4"},{"post_id":"cli5o2dqr0004czzk31si2iet","category_id":"cli5o2dqy000cczzk104y5jt1","_id":"cli5o2dr1000lczzk4zeo3kxp"},{"post_id":"cli5o2dqs0005czzkdltx85at","category_id":"cli5o2dr0000hczzk3hvqag0i","_id":"cli5o2dr2000pczzkh1734qm4"},{"post_id":"cli5o2dqt0006czzk2g2r69sz","category_id":"cli5o2dr1000mczzkfide0ixr","_id":"cli5o2dr3000tczzk6ngf9une"},{"post_id":"cli5o2dqx000bczzk3x63g4lt","category_id":"cli5o2dr2000qczzk6f58dbhq","_id":"cli5o2dr4000wczzkgnfabtd4"}],"PostTag":[{"post_id":"cli5o2dqf0000czzkdml80vjp","tag_id":"cli5o2dqq0003czzk5po65vw4","_id":"cli5o2dqw0009czzkeh62dpoj"},{"post_id":"cli5o2dql0001czzkckq1fhn1","tag_id":"cli5o2dqu0008czzk4igm139w","_id":"cli5o2dr0000fczzk8ks45k92"},{"post_id":"cli5o2dqr0004czzk31si2iet","tag_id":"cli5o2dqz000eczzk9t1rf2gb","_id":"cli5o2dr1000kczzk7ggg018p"},{"post_id":"cli5o2dqs0005czzkdltx85at","tag_id":"cli5o2dr0000iczzk93huhjxr","_id":"cli5o2dr2000oczzk06ub45p5"},{"post_id":"cli5o2dqt0006czzk2g2r69sz","tag_id":"cli5o2dr1000nczzke8vngj67","_id":"cli5o2dr3000sczzk23wyejb0"},{"post_id":"cli5o2dqw000aczzk5il8cl0s","tag_id":"cli5o2dr3000rczzk4aoudugz","_id":"cli5o2dr4000vczzk2c3gb0my"},{"post_id":"cli5o2dqx000bczzk3x63g4lt","tag_id":"cli5o2dr4000uczzkcz7a0pnj","_id":"cli5o2dr4000xczzk46o5ff0b"}],"Tag":[{"name":"lock file","_id":"cli5o2dqq0003czzk5po65vw4"},{"name":"性能 时间戳","_id":"cli5o2dqu0008czzk4igm139w"},{"name":"spring transaction","_id":"cli5o2dqz000eczzk9t1rf2gb"},{"name":"websocket","_id":"cli5o2dr0000iczzk93huhjxr"},{"name":"design view","_id":"cli5o2dr1000nczzke8vngj67"},{"name":"qps ratelimiter","_id":"cli5o2dr3000rczzk4aoudugz"},{"name":"saas 996","_id":"cli5o2dr4000uczzkcz7a0pnj"}]}}